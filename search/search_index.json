{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Any time I work on something that I feel I\u2019ve had to hunt around for information on and that I\u2019ve not really found something that fits what I was looking for I tend to want to write up what I\u2019ve found and put it out there for someone to find \u2013 if only because it\u2019s more than likely me that\u2019ll come back later and find it! To this end I\u2019ve finally got a content publishing tool I like working with and have been trying to put out some stuff out there. You're welcome to browse around and you'll find hopelessly hopeful drafts of articles and other bobbins and gubbins I want to make sure I don't forget. I'll keep trying to organise them into a structure that makes sense... Created with MkDocs and Material theme by squidfunk","title":"Home"},{"location":"about/","text":"Exams and Credentials \u00b6 My Credly Profile LinkedIn \u00b6 Bunkei Ka StackExchange \u00b6 Work \u00b6","title":"About Me"},{"location":"about/#exams-and-credentials","text":"My Credly Profile","title":"Exams and Credentials"},{"location":"about/#linkedin","text":"Bunkei Ka","title":"LinkedIn"},{"location":"about/#stackexchange","text":"","title":"StackExchange"},{"location":"about/#work","text":"","title":"Work"},{"location":"whats-new/","text":"Apr 2021 \u00b6 Embarassed that I keep having to DuckDuckGo this but found a really good simple explanation here How to update a forked repo from an upstream with git rebase (or merge) another good on I found is here Aug 2020 \u00b6 Recent Listening \u00b6 I just rediscovered across this Dub album that was in my collection as a teenager: Tunes From The Missing Channel by Dub Syndicate May 2020 \u00b6 I just discovered Azure Static Web Apps - this might well prompt me to migrate away from using GitHub Pages for publishing this site... Still trying to catch up on some of the news from Microsoft Build 2020 mybuild.microsoft.com/ channel9.msdn.com/Events/Build/2020 youtube.com/c/microsoftdeveloper There was some good coverage here: arstechnica.com/gadgets/2020/05/microsofts-romance-with-open-source-software-on-display-at-build-2020/ In terms of training and learning I found that I'd happily already completed 100% on the Build Cloud Skills Challenge - Azure DevOps . I really need to crack on and re-book an online proctored exam for Exam AZ-400 - Designing and Implementing Microsoft DevOps Solutions to finally gain my https://docs.microsoft.com/en-gb/learn/certifications/devops-engineer . A free exam voucher is available for redemption starting June 8, 2020 and is valid until January 31, 2021 My colleage @d0ugs mode some notes that have prompted me to go back and watch this Building Azure Apps using the Common Data Service Also annouced into public preview - Azure Synapse Analytics is the 'Data Lakehouse' we've all (apparently) been waiting for: youtu.be/HvEi13pWNps - it's an all-in-one studio for everything Data April 2020 \u00b6 Reading \u00b6 How to Work from Home \u00b6 I've promised myself that I'm going to read this today... Did a global pandemic spur your bosses to allow you to work from home? Here's a guide to working from home effectively for those who are just starting out. From Scott Hanselman - Take Remote Worker/Educator webcam video calls to the next level with OBS, NDI Tools, and Elgato Stream Deck Tools \u00b6 Kubeval \u00b6 Kubeval can be used to validate Kubernetes configuration files and Helm charts (e.g. for catching resource namespaces changes when there's a new Kubernetes version). To install to windows using Scoop scoop bucket add instrumenta https :// github . com / instrumenta / bucket-instrumenta scoop install kubeval Zeplin \u00b6 Zeplin is a tool for 'curating designs in the cloud' as a team. Share, organize and collaborate on designs 'built with developers in mind'. Career Tools \u00b6 My pal Martin Short shared this with me this week: 10 Extraordinary GitHub Repos for All Developers I particularly like, and need to go back to the System Design Primer . Dependabot \u00b6 Add Dependabot to your repo for automated dependency updates. it checks your dependency files for outdated requirements and opens individual PRs for any it finds. This repo contains some projects with outdated dependencies. Fork it to try out Dependabot! Downtime \u00b6 While I'm working from home, I'm drinking lots more coffee so I've gone with this!: SEASONAL SUBSCRIPTION \u2014 Fortitude Coffee One 250g bag of our seasonal coffee choice delivered to your door every week.","title":"What's New"},{"location":"whats-new/#apr-2021","text":"Embarassed that I keep having to DuckDuckGo this but found a really good simple explanation here How to update a forked repo from an upstream with git rebase (or merge) another good on I found is here","title":"Apr 2021"},{"location":"whats-new/#aug-2020","text":"","title":"Aug 2020"},{"location":"whats-new/#recent-listening","text":"I just rediscovered across this Dub album that was in my collection as a teenager: Tunes From The Missing Channel by Dub Syndicate","title":"Recent Listening"},{"location":"whats-new/#may-2020","text":"I just discovered Azure Static Web Apps - this might well prompt me to migrate away from using GitHub Pages for publishing this site... Still trying to catch up on some of the news from Microsoft Build 2020 mybuild.microsoft.com/ channel9.msdn.com/Events/Build/2020 youtube.com/c/microsoftdeveloper There was some good coverage here: arstechnica.com/gadgets/2020/05/microsofts-romance-with-open-source-software-on-display-at-build-2020/ In terms of training and learning I found that I'd happily already completed 100% on the Build Cloud Skills Challenge - Azure DevOps . I really need to crack on and re-book an online proctored exam for Exam AZ-400 - Designing and Implementing Microsoft DevOps Solutions to finally gain my https://docs.microsoft.com/en-gb/learn/certifications/devops-engineer . A free exam voucher is available for redemption starting June 8, 2020 and is valid until January 31, 2021 My colleage @d0ugs mode some notes that have prompted me to go back and watch this Building Azure Apps using the Common Data Service Also annouced into public preview - Azure Synapse Analytics is the 'Data Lakehouse' we've all (apparently) been waiting for: youtu.be/HvEi13pWNps - it's an all-in-one studio for everything Data","title":"May 2020"},{"location":"whats-new/#april-2020","text":"","title":"April 2020"},{"location":"whats-new/#reading","text":"","title":"Reading"},{"location":"whats-new/#how-to-work-from-home","text":"I've promised myself that I'm going to read this today... Did a global pandemic spur your bosses to allow you to work from home? Here's a guide to working from home effectively for those who are just starting out. From Scott Hanselman - Take Remote Worker/Educator webcam video calls to the next level with OBS, NDI Tools, and Elgato Stream Deck","title":"How to Work from Home"},{"location":"whats-new/#tools","text":"","title":"Tools"},{"location":"whats-new/#kubeval","text":"Kubeval can be used to validate Kubernetes configuration files and Helm charts (e.g. for catching resource namespaces changes when there's a new Kubernetes version). To install to windows using Scoop scoop bucket add instrumenta https :// github . com / instrumenta / bucket-instrumenta scoop install kubeval","title":"Kubeval"},{"location":"whats-new/#zeplin","text":"Zeplin is a tool for 'curating designs in the cloud' as a team. Share, organize and collaborate on designs 'built with developers in mind'.","title":"Zeplin"},{"location":"whats-new/#career-tools","text":"My pal Martin Short shared this with me this week: 10 Extraordinary GitHub Repos for All Developers I particularly like, and need to go back to the System Design Primer .","title":"Career Tools"},{"location":"whats-new/#dependabot","text":"Add Dependabot to your repo for automated dependency updates. it checks your dependency files for outdated requirements and opens individual PRs for any it finds. This repo contains some projects with outdated dependencies. Fork it to try out Dependabot!","title":"Dependabot"},{"location":"whats-new/#downtime","text":"While I'm working from home, I'm drinking lots more coffee so I've gone with this!: SEASONAL SUBSCRIPTION \u2014 Fortitude Coffee One 250g bag of our seasonal coffee choice delivered to your door every week.","title":"Downtime"},{"location":"_snippets/snip1/","text":"from snip 1 \u00b6 from snip 1","title":"Snip1"},{"location":"_snippets/snip1/#from-snip-1","text":"from snip 1","title":"from snip 1"},{"location":"_snippets/snip2/","text":"from snip 2 \u00b6 from snip 2","title":"Snip2"},{"location":"_snippets/snip2/#from-snip-2","text":"from snip 2","title":"from snip 2"},{"location":"aks/create-tls-secret/","text":"I followed the command-line method (the first method) explained in this article Creating Kubernetes Secrets Using TLS/SSL as an Example - i.e. rather than using the second (YAML file) method. In order to do this I needed two files in the correct format. There are multiple formats that certificate and associated key files can be in (they can even be combined into a single file). In order to create a Kubernetes TLS secret I needed to ascertain the right ones to use. I was provided with a .pks file and needed to work out how to generate the correct artifacts from it. All I knew from the above article was that I needed a .crt and a .key file and at first I wasn't sure what these were. What are we trying to achieve? \u00b6 I needed obtain a TLS certificate and create a TLS Secret object in Kubernetes so that an Ingress resource could refer to the Secret in order that the certificate presented by the NGINX Ingress would look like this when visiting the associated Service in a web browser: (i.e. the TLS/HTTPS certificate should include the CA chain.) Getting the right files \u00b6 To get an encrypted private key: openssl pkcs12 -in domain.pfx -nocerts -out domain.enc.key To get an un-encrypted private key file openssl rsa -in domain.enc.key -outform PEM -out domain.key Note: be sure to delete this file once uploaded to the cluster so that you don't have an un-encrypted secret on your local machine To get the certificate file openssl pkcs12 -in domain.pfx -nodes -nokeys -nomac -out domain.crt The domain.crt file looks like this In my case it contains the full CA chain and so, in my case, there are three certificates each enclosed in BEGIN CERTIFICATE and END CERTIFICATE delimiters: -----BEGIN CERTIFICATE----- # ############################################################### # ############################################################### -----END CERTIFICATE----- Creating the TLS Secret in Kubernetes \u00b6 Create Kubernetes TLS Secret: kubectl create secret tls tlscert --key=\"tls.key\" --cert=\"tls.crt\" Additional Notes \u00b6 How to validate a key \u00b6 This works for both encrypted or un-encrypted keys : openssl rsa -check -in domain.enc.key or openssl rsa -check -in domain.key You should see the message 'RSA key ok': $ openssl rsa -check -in domain.enc.key Enter pass phrase for domain.enc.pem: RSA key ok writing RSA key -----BEGIN RSA PRIVATE KEY----- # ############################################################### # ############################################################### Where did my PKS file come from? \u00b6 I was provided with a .pks ( PKCS#12 ) file which had been created with the following command openssl pkcs12 -export -out domain.pfx -inkey domain.rsa -in domain.cer -certfile CAbundle.txt Where: domain.cer is the client certificate (i.e. without the Certification Authority (CA) chain ) domain.rsa is an un-encrypted version of the private key CAbundle.txt contains the CA certificates to append to create the CA chain domain.cer looks like this As you can see if contains just a single certificate domain.rsa looks like this How to get the client certificate (without the full CA chain) \u00b6 openssl pkcs12 -in domain.pfx -clcerts -nokeys -out domain.cer Some of the steps in this article are based on How to convert a PFX to a seperate .key/.crt file - by Mark Brilman I also referred to the OpenSSL PKCS12 man pages And to OpenSSL Essentials: Working with SSL Certificates, Private Keys and CSRs Updates \u00b6 While following an MS Learn module I found this article on using Azure Key Vault to do much of the above: Manage certificates \"...you can connect your Key Vault with a trusted certificate issuer (referred to as an integrated CA) and create the certificate directly in Azure Key Vault. You can then request to create a certificate and the Key Vault will interact directly with the CA to fulfill the request\" Alternatively, you can also just use Key Vault to create self-signed certificates for testing, or to create an X.509 certificate signing request (CSR) to pass on to the certificate authority (CA) to process and then later request Key Vault to merge the resulting X.509 certificate with the key pair held in Key Vault.","title":"How to Create a TLS Secret"},{"location":"aks/create-tls-secret/#what-are-we-trying-to-achieve","text":"I needed obtain a TLS certificate and create a TLS Secret object in Kubernetes so that an Ingress resource could refer to the Secret in order that the certificate presented by the NGINX Ingress would look like this when visiting the associated Service in a web browser: (i.e. the TLS/HTTPS certificate should include the CA chain.)","title":"What are we trying to achieve?"},{"location":"aks/create-tls-secret/#getting-the-right-files","text":"To get an encrypted private key: openssl pkcs12 -in domain.pfx -nocerts -out domain.enc.key To get an un-encrypted private key file openssl rsa -in domain.enc.key -outform PEM -out domain.key Note: be sure to delete this file once uploaded to the cluster so that you don't have an un-encrypted secret on your local machine To get the certificate file openssl pkcs12 -in domain.pfx -nodes -nokeys -nomac -out domain.crt The domain.crt file looks like this In my case it contains the full CA chain and so, in my case, there are three certificates each enclosed in BEGIN CERTIFICATE and END CERTIFICATE delimiters: -----BEGIN CERTIFICATE----- # ############################################################### # ############################################################### -----END CERTIFICATE-----","title":"Getting the right files"},{"location":"aks/create-tls-secret/#creating-the-tls-secret-in-kubernetes","text":"Create Kubernetes TLS Secret: kubectl create secret tls tlscert --key=\"tls.key\" --cert=\"tls.crt\"","title":"Creating the TLS Secret in Kubernetes"},{"location":"aks/create-tls-secret/#additional-notes","text":"","title":"Additional Notes"},{"location":"aks/create-tls-secret/#how-to-validate-a-key","text":"This works for both encrypted or un-encrypted keys : openssl rsa -check -in domain.enc.key or openssl rsa -check -in domain.key You should see the message 'RSA key ok': $ openssl rsa -check -in domain.enc.key Enter pass phrase for domain.enc.pem: RSA key ok writing RSA key -----BEGIN RSA PRIVATE KEY----- # ############################################################### # ###############################################################","title":"How to validate a key"},{"location":"aks/create-tls-secret/#where-did-my-pks-file-come-from","text":"I was provided with a .pks ( PKCS#12 ) file which had been created with the following command openssl pkcs12 -export -out domain.pfx -inkey domain.rsa -in domain.cer -certfile CAbundle.txt Where: domain.cer is the client certificate (i.e. without the Certification Authority (CA) chain ) domain.rsa is an un-encrypted version of the private key CAbundle.txt contains the CA certificates to append to create the CA chain domain.cer looks like this As you can see if contains just a single certificate domain.rsa looks like this","title":"Where did my PKS file come from?"},{"location":"aks/create-tls-secret/#how-to-get-the-client-certificate-without-the-full-ca-chain","text":"openssl pkcs12 -in domain.pfx -clcerts -nokeys -out domain.cer Some of the steps in this article are based on How to convert a PFX to a seperate .key/.crt file - by Mark Brilman I also referred to the OpenSSL PKCS12 man pages And to OpenSSL Essentials: Working with SSL Certificates, Private Keys and CSRs","title":"How to get the client certificate (without the full CA chain)"},{"location":"aks/create-tls-secret/#updates","text":"While following an MS Learn module I found this article on using Azure Key Vault to do much of the above: Manage certificates \"...you can connect your Key Vault with a trusted certificate issuer (referred to as an integrated CA) and create the certificate directly in Azure Key Vault. You can then request to create a certificate and the Key Vault will interact directly with the CA to fulfill the request\" Alternatively, you can also just use Key Vault to create self-signed certificates for testing, or to create an X.509 certificate signing request (CSR) to pass on to the certificate authority (CA) to process and then later request Key Vault to merge the resulting X.509 certificate with the key pair held in Key Vault.","title":"Updates"},{"location":"aks/domains/","text":"Clusters of Domains - designing domains and DNS for AKS \u00b6 While the documentation on Azure DNS (at the time of writing in early 2020)) covers the essentials of what you need to know I personally found it somewhat dry in that it assumed that you knew what you wanted to do with Zones and records and mostly explained how to do create and configure each component piece without providing much context about when and where you need them and how you combine them into a solution to meet requirements. I was finding it initially difficult to synthesise and apply the pieces of information and there was a lack of something to place it all into a real-world solution-design context. For example, how should I expose a suite of web applications or microservices and their APIs to the public internet whilst supporting a 'route to live' set of environments into which to deploy these artefacts so that they can be tested while they are under development. In other words: How can I efficiently use domain names to support Development, Testing, Staging, and Production Environments? Should I use a separate domain name for each Environment? Do I need to have a separate Kubernetes cluster for each or can I combine custom domain names with one or more clusters in any combination? I'll share what I've found so far and hope to answer the above questions. Note While my original design conundrum arose in how to use a custom domain name with one or more AKS clusters, this article will contain information relavant to using domain names with Azure Resources generally If at first you don't succeed - Play! \u00b6 I knew I wanted to build a solution that would combine cert-manager and external-dns controllers in each Kubernetes cluster so that applications could be deployed without the Application Developers needing to manage either DNS records nor handling certificates or key pairs in deployment pipelines, thus also avoiding any risk of inadvertantly exposing secrets and certificates in source code repositories. My problem was that none of the documentation I could find brought all of these pieces together in a multi-environment solution. I could get pieces working in isolation but how to bring them all together? Info The external-dns project This can be installed into a cluster and it watches deployments for any that require DNS records to be created or updated and it manages records in an associated Azure DNS Zone resource. Info The cert-manager project This is a native Kubernetes certificate management controller which can be used to issue certificates automatically. It ensures Initially I felt like I was going round in circles simply trying to get a 'multi-environment' design to occur to me by simply reading the documentation for AKS and for these other open-source components. The break-through came when starting to implement something simple and trying to build towards a more complex solution (I simply didn't know where I was going before I set off - no compass, no map). Trying out cert-manager \u00b6 Since I didn't have a custom domain name I could use I attempted to use the Azure-assigned fully qualified domain name (FQDN). Any AKS cluster into which a Service of type LoadBalancer has been deployed will have a Public IP resource deployed into the cluster-managed Resource Group. Any Azure Public IP (PIP) resource can optionally have a DNS name label applied which will then mean that this address will resolve to the IP address. So that seemed like the simplest way to test cert-manager . Info The documentation states it best: \"You can specify a DNS domain name label for a public IP resource, which creates a mapping for domainnamelabel.location.cloudapp.azure.com to the public IP address in the Azure-managed DNS servers\" from: IP address types and allocation methods in Azure > DNS hostname resolution So, in my case, given that adding a name label of contosoapps to my PIP the FQDN is therefore contosoapps.westeurope.cloudapp.azure.com . Once this was working I was able to host multiple services, each with an Ingress based on the FQDN. So, for example, the demo-nginx app could be reached with the URL demo-nqinx.contosoapps.westeurope.cloudapp.azure.com . Application Name FQDN demo-nginx demo-nqinx.contosoapps.westeurope.cloudapp.azure.com demo-aspdotnet demo-aspdotnet.contosoapps.westeurope.cloudapp.azure.com But what if I also want to support multiple deployment environments? Application Name Environment URL demo-nginx Development demo-nqinx.**dev* .contosoapps.westeurope.cloudapp.azure.com** demo-nginx Staging demo-nqinx.**test* .contosoapps.westeurope.cloudapp.azure.com** demo-nginx Production demo-nqinx .contosoapps.westeurope.cloudapp.azure.com Unfortunately those URLs marked with * indicate that we cannot do this. Possible variations of the above that I considered were: demo-nqinx. dev ... dev .demo-nqinx... dev -demo-nqinx... I could not find this documented anywhere but it seems that name labels on Public IP Addresses only support a single label and so we cannot use FQDNS with anything other than 6-part names. In the first two examples above I'm trying to use 7-part names and in the third example I would need more than one name label on the IP address - one for each environment. When in doubt - Puchase a Custom Domain Name! \u00b6 So it was clear that I needed to purchase my own Custom Domain Name in order to fully explore and model a realistic scenario with multiple deployment environments. I registered a cheap domain I could use for testing for sake of this article I'll use the name contosoapps.xyz ). Once I have a domain I'll somehow need to use an Azure DNS Zone and DNS records in combination with Kubernetes Ingress Controller and Resources to expose a suite of web applications or microservices and their APIs to the public internet. Purpose Recordset Name Record Type Alias Resource Type Alias Resource Target Production @ NS Development Environment Ingress dev A Public IP Address LoadBalancer Service Public IP Staging Environment Ingress staging A Public IP Address LoadBalancer Service Public IP This model uses a single DNS Zone resource with an alias record for each Environment we wish to represent. This model works fine for a solution where we only want to use cert-manager is too simplistic if we also want to use external-dns to manage DNS records automatically without knowing all required names ahead of time. Trying out external-dns \u00b6 The only Microsoft documentation I could find covering externals-dns only applies to a cluster where the HTTP application routing add-on has been enabled and given that this add-on is not recommended for production use I wanted to avoiding this add-on. Info The HTTP application routing add-on automatically creates a DNS Zone for you and creates two controllers inside the cluster (an Ingress controller and External-DNS controller) and I wanted to understand how to deploy and manage these components properly without it happening 'auto-magically'! Eventually I was able to piece together the elements of a solution for using external-dns in AKS from this page (albeit not using Rancher in my case). Again this provided a working demonstration of external-dns on it's own but not in a multi-environment deployment context. My problem is that with external-dns the DNS Zone resource represents the first two parts of my domain name. In this example, my Apex is contosoapp.com which has two parts, and external-dns controller would only manage part three by adding A-records such as dev-app and staging-app ) but we'll be limited to three-part names. Hierarchy to the rescue! \u00b6 This image gave me an idea!: from: docs.microsoft.com/en-us/azure/dns/dns-domain-delegation Why not have a hierachy of DNS zone resources?! In a hierarchy of DNS Zone resources a 'parent' one delegates to one or more 'child' DNS Zones. This would enable us to implement my multi-environment model whereby each DNS Zone would be an environment ( dev.contosoapps.com , staging.contosoapps.com ) and the parent is the production environment ( contosoapps.com ) I actually tried this out before I'd identified the area of the documentation that explains how to do it: Delegate an Azure DNS subdomain Once I'd got the idea of DNS zones referencing each other somehow, I went hunting for examples of what the Azure CLI commands for doing this might look like. Whilst searching for az network dns zone I came across a tiny snippet of code here: jwendl.net/code-notes/azure/network/ which hinted at what I needed to do, so I went ahead and tried it. My set of commands to acheive this in Azure CLI this is in PowerShell rather then Bash incidentally : $dnsRgName = 'RG-DNS-Zones' $domainName = 'contosoapps.xyz' $dnsZoneId =$( az network dns zone create -g $dnsRgName -n $domainName - -query id -o tsv ) $domainNameSub = 'dev.contosoapps.xyz' $dnsZoneIdSub =$( az network dns zone create -g $dnsRgName -n $domainNameSub - -query id -o tsv ) # list name servers for dev (subdomain) zone $ns0 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[0]\" -o tsv ) $ns1 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[1]\" -o tsv ) $ns2 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[2]\" -o tsv ) $ns3 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[3]\" -o tsv ) # add subdomain NS records to apex zone az network dns record-set ns create -g $dnsRgName -z $domainName -n 'dev' az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns0 az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns1 az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns2 az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns3 Info ** Delegating my custom domain ** As described in the documentation Tutorial: Host your domain in Azure DNS : \"In the registrar's DNS management page, edit the NS records and replace the NS records with the Azure DNS name servers.\" In my case this was with 123-reg.co.uk and looks like this: A DNS Zone Per environment \u00b6 With this solution in place we should now be free to allow an instance of external-dns manage each DNS Zone resource separately. Note In Progress I've yet to actually try this last but - the plan is to flesh this article out step-by-step as I keep trying things!","title":"Clusters of Domains"},{"location":"aks/domains/#clusters-of-domains-designing-domains-and-dns-for-aks","text":"While the documentation on Azure DNS (at the time of writing in early 2020)) covers the essentials of what you need to know I personally found it somewhat dry in that it assumed that you knew what you wanted to do with Zones and records and mostly explained how to do create and configure each component piece without providing much context about when and where you need them and how you combine them into a solution to meet requirements. I was finding it initially difficult to synthesise and apply the pieces of information and there was a lack of something to place it all into a real-world solution-design context. For example, how should I expose a suite of web applications or microservices and their APIs to the public internet whilst supporting a 'route to live' set of environments into which to deploy these artefacts so that they can be tested while they are under development. In other words: How can I efficiently use domain names to support Development, Testing, Staging, and Production Environments? Should I use a separate domain name for each Environment? Do I need to have a separate Kubernetes cluster for each or can I combine custom domain names with one or more clusters in any combination? I'll share what I've found so far and hope to answer the above questions. Note While my original design conundrum arose in how to use a custom domain name with one or more AKS clusters, this article will contain information relavant to using domain names with Azure Resources generally","title":"Clusters of Domains - designing domains and DNS for AKS"},{"location":"aks/domains/#if-at-first-you-dont-succeed-play","text":"I knew I wanted to build a solution that would combine cert-manager and external-dns controllers in each Kubernetes cluster so that applications could be deployed without the Application Developers needing to manage either DNS records nor handling certificates or key pairs in deployment pipelines, thus also avoiding any risk of inadvertantly exposing secrets and certificates in source code repositories. My problem was that none of the documentation I could find brought all of these pieces together in a multi-environment solution. I could get pieces working in isolation but how to bring them all together? Info The external-dns project This can be installed into a cluster and it watches deployments for any that require DNS records to be created or updated and it manages records in an associated Azure DNS Zone resource. Info The cert-manager project This is a native Kubernetes certificate management controller which can be used to issue certificates automatically. It ensures Initially I felt like I was going round in circles simply trying to get a 'multi-environment' design to occur to me by simply reading the documentation for AKS and for these other open-source components. The break-through came when starting to implement something simple and trying to build towards a more complex solution (I simply didn't know where I was going before I set off - no compass, no map).","title":"If at first you don't succeed - Play!"},{"location":"aks/domains/#trying-out-cert-manager","text":"Since I didn't have a custom domain name I could use I attempted to use the Azure-assigned fully qualified domain name (FQDN). Any AKS cluster into which a Service of type LoadBalancer has been deployed will have a Public IP resource deployed into the cluster-managed Resource Group. Any Azure Public IP (PIP) resource can optionally have a DNS name label applied which will then mean that this address will resolve to the IP address. So that seemed like the simplest way to test cert-manager . Info The documentation states it best: \"You can specify a DNS domain name label for a public IP resource, which creates a mapping for domainnamelabel.location.cloudapp.azure.com to the public IP address in the Azure-managed DNS servers\" from: IP address types and allocation methods in Azure > DNS hostname resolution So, in my case, given that adding a name label of contosoapps to my PIP the FQDN is therefore contosoapps.westeurope.cloudapp.azure.com . Once this was working I was able to host multiple services, each with an Ingress based on the FQDN. So, for example, the demo-nginx app could be reached with the URL demo-nqinx.contosoapps.westeurope.cloudapp.azure.com . Application Name FQDN demo-nginx demo-nqinx.contosoapps.westeurope.cloudapp.azure.com demo-aspdotnet demo-aspdotnet.contosoapps.westeurope.cloudapp.azure.com But what if I also want to support multiple deployment environments? Application Name Environment URL demo-nginx Development demo-nqinx.**dev* .contosoapps.westeurope.cloudapp.azure.com** demo-nginx Staging demo-nqinx.**test* .contosoapps.westeurope.cloudapp.azure.com** demo-nginx Production demo-nqinx .contosoapps.westeurope.cloudapp.azure.com Unfortunately those URLs marked with * indicate that we cannot do this. Possible variations of the above that I considered were: demo-nqinx. dev ... dev .demo-nqinx... dev -demo-nqinx... I could not find this documented anywhere but it seems that name labels on Public IP Addresses only support a single label and so we cannot use FQDNS with anything other than 6-part names. In the first two examples above I'm trying to use 7-part names and in the third example I would need more than one name label on the IP address - one for each environment.","title":"Trying out cert-manager"},{"location":"aks/domains/#when-in-doubt-puchase-a-custom-domain-name","text":"So it was clear that I needed to purchase my own Custom Domain Name in order to fully explore and model a realistic scenario with multiple deployment environments. I registered a cheap domain I could use for testing for sake of this article I'll use the name contosoapps.xyz ). Once I have a domain I'll somehow need to use an Azure DNS Zone and DNS records in combination with Kubernetes Ingress Controller and Resources to expose a suite of web applications or microservices and their APIs to the public internet. Purpose Recordset Name Record Type Alias Resource Type Alias Resource Target Production @ NS Development Environment Ingress dev A Public IP Address LoadBalancer Service Public IP Staging Environment Ingress staging A Public IP Address LoadBalancer Service Public IP This model uses a single DNS Zone resource with an alias record for each Environment we wish to represent. This model works fine for a solution where we only want to use cert-manager is too simplistic if we also want to use external-dns to manage DNS records automatically without knowing all required names ahead of time.","title":"When in doubt - Puchase a Custom Domain Name!"},{"location":"aks/domains/#trying-out-external-dns","text":"The only Microsoft documentation I could find covering externals-dns only applies to a cluster where the HTTP application routing add-on has been enabled and given that this add-on is not recommended for production use I wanted to avoiding this add-on. Info The HTTP application routing add-on automatically creates a DNS Zone for you and creates two controllers inside the cluster (an Ingress controller and External-DNS controller) and I wanted to understand how to deploy and manage these components properly without it happening 'auto-magically'! Eventually I was able to piece together the elements of a solution for using external-dns in AKS from this page (albeit not using Rancher in my case). Again this provided a working demonstration of external-dns on it's own but not in a multi-environment deployment context. My problem is that with external-dns the DNS Zone resource represents the first two parts of my domain name. In this example, my Apex is contosoapp.com which has two parts, and external-dns controller would only manage part three by adding A-records such as dev-app and staging-app ) but we'll be limited to three-part names.","title":"Trying out external-dns"},{"location":"aks/domains/#hierarchy-to-the-rescue","text":"This image gave me an idea!: from: docs.microsoft.com/en-us/azure/dns/dns-domain-delegation Why not have a hierachy of DNS zone resources?! In a hierarchy of DNS Zone resources a 'parent' one delegates to one or more 'child' DNS Zones. This would enable us to implement my multi-environment model whereby each DNS Zone would be an environment ( dev.contosoapps.com , staging.contosoapps.com ) and the parent is the production environment ( contosoapps.com ) I actually tried this out before I'd identified the area of the documentation that explains how to do it: Delegate an Azure DNS subdomain Once I'd got the idea of DNS zones referencing each other somehow, I went hunting for examples of what the Azure CLI commands for doing this might look like. Whilst searching for az network dns zone I came across a tiny snippet of code here: jwendl.net/code-notes/azure/network/ which hinted at what I needed to do, so I went ahead and tried it. My set of commands to acheive this in Azure CLI this is in PowerShell rather then Bash incidentally : $dnsRgName = 'RG-DNS-Zones' $domainName = 'contosoapps.xyz' $dnsZoneId =$( az network dns zone create -g $dnsRgName -n $domainName - -query id -o tsv ) $domainNameSub = 'dev.contosoapps.xyz' $dnsZoneIdSub =$( az network dns zone create -g $dnsRgName -n $domainNameSub - -query id -o tsv ) # list name servers for dev (subdomain) zone $ns0 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[0]\" -o tsv ) $ns1 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[1]\" -o tsv ) $ns2 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[2]\" -o tsv ) $ns3 =$( az network dns record-set ns show - -resource-group $dnsRgName - -zone-name $domainNameSub - -name '@' - -query \"nsRecords[3]\" -o tsv ) # add subdomain NS records to apex zone az network dns record-set ns create -g $dnsRgName -z $domainName -n 'dev' az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns0 az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns1 az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns2 az network dns record-set ns add-record -g $dnsRgName -z $domainName - -record-set-name 'dev' - -nsdname $ns3 Info ** Delegating my custom domain ** As described in the documentation Tutorial: Host your domain in Azure DNS : \"In the registrar's DNS management page, edit the NS records and replace the NS records with the Azure DNS name servers.\" In my case this was with 123-reg.co.uk and looks like this:","title":"Hierarchy to the rescue!"},{"location":"aks/domains/#a-dns-zone-per-environment","text":"With this solution in place we should now be free to allow an instance of external-dns manage each DNS Zone resource separately. Note In Progress I've yet to actually try this last but - the plan is to flesh this article out step-by-step as I keep trying things!","title":"A DNS Zone Per environment"},{"location":"compute/patching-devtestlabs/","text":"We regularly help our customers to use Azure's DevTest Labs in combination with Data Science Virtual Machines to safely and securely spin up the required resources for rapid-fire projects without slowing down the pace of innovation and time-to-market for new ideas. One aspect of providing a secure environment is ensure that machines are regularly patched and operating systems and software kept up to date to limit the risk of vulnerabity to expliots and fix bugs. At the time of writing, I'm part of a small team supporting a variety of teams in various parts of the business who need access to such environments at short notice and we don't have the time to be patching virtual machines one-by-one on a routine basis. As a result we need to use as much automation as we can to do this for us. This article is about how we used Azure's Update Management solution to keep Virtual Mchines in DevTest Labs up to date with very little manual intervention from the administrators ( that's us! ). About DevTest Labs \u00b6 DevTest Labs (DTL) enable project team members to self-manage virtual machines (VMs) without waiting for approvals, or for an administrator to create and configure it for them. This allows us to offer pre-configured base machines (both Windows and Linux) which have all the necessary tools and software built-in which users can then spin up in just a few minutes. Secure Environment \u00b6 VMs are made avaiable in a DTL and depending on requirements we typically be protect them as follows: Placing the VMs into a subnet within a private Virtual Network (VNET) so that a perimeter firewall device can be used to filter and log internet and intra-network access. Applying a User Defined Route (UDR) to ensure that all traffic from the subnet traverses the firewall. The DTL works on our behalf to ensure that each of the virtual machines and its associated resources are deployed into a separate resource group RBAC is used to limit who can change settings or gain access to the virtual machine. The user who has claimed the machine has just enough rights to use it but not modify anything else. Using DevTest Lab Artifacts to add additional components and configure the operating system and machine policy. Using Azure's Update Management solution (see below) to keep operating systems patched. This topic is the main focus of this article. Tools for Data Science \u00b6 Our customers have more and more data and they are asking more a more comlex questions of that data. Developers and Data Scientists are engaged to find the answers and they need to quickly spin up an environment with all the right tools. Data Science Virtual Machines (DSVM) are a customized VM image which we typically make available for end-user selection in our DTL environments. These are VMs built specifically for doing data science. They come pre-installed and pre-configured with many popular data science tools which helps to jump-start data engineering and advanced analytics. The combination of DSVMs and a DevTest Lab Environment is a powerful enabler for people to get started on the job quickly without worrying about precious data assets falling into the wrong hands. Azure Automation - Update Management \u00b6 We use the Update Management solution in Azure Automation to manage operating system updates for virtual machines in our DevTest Labs in Azure. Update Management scans and displays the status of available updates and it manages the process of installing required updates on each machine on a configurable schedule. Here's what we needed: Update Management can only patch a machine whilst it's running Machines are shut down automatically by the DevTest Labs environment out of working hours to save costs and therefore machines are only running during the daytime. We want to limit how much we interrupt a user while they're using their VM so any system maintenance needs to happen out-of-hours. As such we needed to find a way to ensure that virtual machines were running when the patching process starts but are shut down again afterwards to limit costs. Using pre and post-scripts to prepare machines for patching \u00b6 Pre-scripts and post-scripts let you run PowerShell runbooks in your Azure Automation account before (pre-task) and after (post-task) a scheduled update management deployment. The documentation 'Manage pre and post-scripts' provided us with most of what we needed to know. At time of writing the document does a good job of explaining the general concept of pre and post-scripts using script gallery samples such as UpdateManagement-TurnOnVMS and UpdateManagement-TurnOffVMs as examples of what you can do with these kinds of runbooks. In this article I'll fill in a few gaps in the end-to-end steps we needed to take to get these start/stop scripts working for us as a routine patching solution. UpdateManagement-TurnOnVms - ensures all Azure VMs in the Update Deployment are running so they recieve updates. It stores the names of machines that were started in an Automation variable so only those machines are turned back off (by the accompanying UpdateManagement-TurnOffVms script) when the deployment is finished. UpdateManagement-TurnOffVms - ensures that all Azure VMs in the Update Deployment are turned off after they recieve updates by reads the names of the machines from the Automation variable. Source code for the above modules can be found from zjalexander in GitHub . The ReadMe document in GitHub explains that the scripts have the following requirements, which I'll explain in a bit more detail here. Follow these links to create an Automation Account and a linked Log Analytics Workspace with the Update Management solution enabled. A RunAs account for interacting with the Azure services used by these scripts. Follow this document to Create a Run As account either in Azure portal or or with PowerShell. Be aware that you need sufficient privileges (at least Application administrator in Azure Active Directory and an Owner in a subscription) to complete this task. The ThreadJob module imported into your Automation Account. The UpdateManagement modules both depend on the ThreadJob module . This module extends the existing PowerShell BackgroundJob to include a new thread-based job to provide faster operation with less overhead. To install it go to the URL in a browser www.powershellgallery.com/packages/ThreadJob click on the 'Azure Automation' tab click on the button 'Deploy to Azure Automation' pick the Subscription name, Resource Group and Location and select the Automation Account you want to install the module into. The latest versions of the AzureRM modules. In order to Update Azure PowerShell modules in Azure Automation need to download the Update Azure modules runbook from GitHub and import and publish it into the list of Runbooks your Automation Account. Top Tip Rather than download the runbook powershell script from GitHub, you can directly paste the 'raw' file URL from GitHub into the 'Runbook file' textbox i.e. ' raw.githubusercontent.com/microsoft/AzureAutomation-Account-Modules-Update/master/Update-AutomationAzureModulesForAccount.ps1 ' Now you can use the Runbook Gallery to import the start-stop scripts as follows: In the Azure portal, open your Automation account Under Process Automation, click on Runbooks Gallery Select 'Source: Script Center' Enter 'UpdateManagement' into the search box select each module and click 'Import' : Update Management - Turn On VMs Update Management - Turn Off VMs Important After you import runbooks you need to publish them before they can be used. To do this, find the runbook in Runbooks pane and click Edit and Publish . Visit the Docs page for details on how to use these pre and post-scripts in your update deployment . Here's what one of my update deployment runs looks like with the script status showing completed:","title":"VM Patching in DevTest Labs"},{"location":"compute/patching-devtestlabs/#about-devtest-labs","text":"DevTest Labs (DTL) enable project team members to self-manage virtual machines (VMs) without waiting for approvals, or for an administrator to create and configure it for them. This allows us to offer pre-configured base machines (both Windows and Linux) which have all the necessary tools and software built-in which users can then spin up in just a few minutes.","title":"About DevTest Labs"},{"location":"compute/patching-devtestlabs/#secure-environment","text":"VMs are made avaiable in a DTL and depending on requirements we typically be protect them as follows: Placing the VMs into a subnet within a private Virtual Network (VNET) so that a perimeter firewall device can be used to filter and log internet and intra-network access. Applying a User Defined Route (UDR) to ensure that all traffic from the subnet traverses the firewall. The DTL works on our behalf to ensure that each of the virtual machines and its associated resources are deployed into a separate resource group RBAC is used to limit who can change settings or gain access to the virtual machine. The user who has claimed the machine has just enough rights to use it but not modify anything else. Using DevTest Lab Artifacts to add additional components and configure the operating system and machine policy. Using Azure's Update Management solution (see below) to keep operating systems patched. This topic is the main focus of this article.","title":"Secure Environment"},{"location":"compute/patching-devtestlabs/#tools-for-data-science","text":"Our customers have more and more data and they are asking more a more comlex questions of that data. Developers and Data Scientists are engaged to find the answers and they need to quickly spin up an environment with all the right tools. Data Science Virtual Machines (DSVM) are a customized VM image which we typically make available for end-user selection in our DTL environments. These are VMs built specifically for doing data science. They come pre-installed and pre-configured with many popular data science tools which helps to jump-start data engineering and advanced analytics. The combination of DSVMs and a DevTest Lab Environment is a powerful enabler for people to get started on the job quickly without worrying about precious data assets falling into the wrong hands.","title":"Tools for Data Science"},{"location":"compute/patching-devtestlabs/#azure-automation-update-management","text":"We use the Update Management solution in Azure Automation to manage operating system updates for virtual machines in our DevTest Labs in Azure. Update Management scans and displays the status of available updates and it manages the process of installing required updates on each machine on a configurable schedule. Here's what we needed: Update Management can only patch a machine whilst it's running Machines are shut down automatically by the DevTest Labs environment out of working hours to save costs and therefore machines are only running during the daytime. We want to limit how much we interrupt a user while they're using their VM so any system maintenance needs to happen out-of-hours. As such we needed to find a way to ensure that virtual machines were running when the patching process starts but are shut down again afterwards to limit costs.","title":"Azure Automation - Update Management"},{"location":"compute/patching-devtestlabs/#using-pre-and-post-scripts-to-prepare-machines-for-patching","text":"Pre-scripts and post-scripts let you run PowerShell runbooks in your Azure Automation account before (pre-task) and after (post-task) a scheduled update management deployment. The documentation 'Manage pre and post-scripts' provided us with most of what we needed to know. At time of writing the document does a good job of explaining the general concept of pre and post-scripts using script gallery samples such as UpdateManagement-TurnOnVMS and UpdateManagement-TurnOffVMs as examples of what you can do with these kinds of runbooks. In this article I'll fill in a few gaps in the end-to-end steps we needed to take to get these start/stop scripts working for us as a routine patching solution. UpdateManagement-TurnOnVms - ensures all Azure VMs in the Update Deployment are running so they recieve updates. It stores the names of machines that were started in an Automation variable so only those machines are turned back off (by the accompanying UpdateManagement-TurnOffVms script) when the deployment is finished. UpdateManagement-TurnOffVms - ensures that all Azure VMs in the Update Deployment are turned off after they recieve updates by reads the names of the machines from the Automation variable. Source code for the above modules can be found from zjalexander in GitHub . The ReadMe document in GitHub explains that the scripts have the following requirements, which I'll explain in a bit more detail here. Follow these links to create an Automation Account and a linked Log Analytics Workspace with the Update Management solution enabled. A RunAs account for interacting with the Azure services used by these scripts. Follow this document to Create a Run As account either in Azure portal or or with PowerShell. Be aware that you need sufficient privileges (at least Application administrator in Azure Active Directory and an Owner in a subscription) to complete this task. The ThreadJob module imported into your Automation Account. The UpdateManagement modules both depend on the ThreadJob module . This module extends the existing PowerShell BackgroundJob to include a new thread-based job to provide faster operation with less overhead. To install it go to the URL in a browser www.powershellgallery.com/packages/ThreadJob click on the 'Azure Automation' tab click on the button 'Deploy to Azure Automation' pick the Subscription name, Resource Group and Location and select the Automation Account you want to install the module into. The latest versions of the AzureRM modules. In order to Update Azure PowerShell modules in Azure Automation need to download the Update Azure modules runbook from GitHub and import and publish it into the list of Runbooks your Automation Account. Top Tip Rather than download the runbook powershell script from GitHub, you can directly paste the 'raw' file URL from GitHub into the 'Runbook file' textbox i.e. ' raw.githubusercontent.com/microsoft/AzureAutomation-Account-Modules-Update/master/Update-AutomationAzureModulesForAccount.ps1 ' Now you can use the Runbook Gallery to import the start-stop scripts as follows: In the Azure portal, open your Automation account Under Process Automation, click on Runbooks Gallery Select 'Source: Script Center' Enter 'UpdateManagement' into the search box select each module and click 'Import' : Update Management - Turn On VMs Update Management - Turn Off VMs Important After you import runbooks you need to publish them before they can be used. To do this, find the runbook in Runbooks pane and click Edit and Publish . Visit the Docs page for details on how to use these pre and post-scripts in your update deployment . Here's what one of my update deployment runs looks like with the script status showing completed:","title":"Using pre and post-scripts to prepare machines for patching"},{"location":"devops/azdo-self-hosted-build-agents/","text":"Self-Hosted Agents \u00b6 About Build Agents This new Learning Module has some very good explanations: A build agent is a system that performs build tasks. Think of it as a dedicated server that runs your build process. Imagine that you have an Azure Pipelines project that receives build requests many times per day. Or perhaps you have multiple projects that can each use the same type of build agent. You can organize build agents into agent pools to help ensure that there's a server ready to process each build request. When a build is triggered, Azure Pipelines selects an available build agent from the pool. If all agents are busy, the process waits for one to become available. from docs.microsoft.com/en-us/learn/modules/host-build-agent/ When my colleagues needed a build agent that could communicate with Azure Resources (such as Storage Accounts) inside of a private Virtual Network then the only way to do this without whitelisting a very large set of public IP address ranges (i.e. Azure Datacenter addresses) was to have a Virtual Machine running inside the same private network to which Azure DevOps would schedule build jobs. Azure DevOps provides a way for you to run your own Build Agents called Self-hosted Agents . Configuring an agent on Ubuntu Server \u00b6 When I learned that the requirement was to build Python wheels to be deployed. I guessed that a Linux Virtual Machine should suffice and be a bit less costly to run that a Windows Server Virtual Machine. I elected to deploy a Ubuntu Server. When I looked into this I found a few blog posts in addition to the official documentation , but all the articles specified using a desktop browser to download the Agent configuration script, and this seemed very odd for Linux where I'd assumed the default mode would be to do everything at the command line so I thought I'd post here how I acheived all of this with just an SSH terminal. Step 0. Pre-requisites \u00b6 Update the apt package manager cache sudo apt-get update Step 1. Create an account under which to run the Agent \u00b6 sudo adduser agentsvc sudo usermod -aG sudo agentsvc # test it su - agentsvc sudo ls -la /root Step 2. Install Git2 \u00b6 sudo add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install git Step 3. Install and configure the Agent \u00b6 mkdir -p $HOME /Downloads && cd $HOME /Downloads curl -o $HOME /Downloads/vsts-agent-linux-x64-2.165.0.tar.gz https://vstsagentpackage.azureedge.net/agent/2.165.0/vsts-agent-linux-x64-2.165.0.tar.gz cd $HOME mkdir -p $HOME /myagent/ && cd $HOME /myagent/ tar zxvf $HOME /Downloads/vsts-agent-linux-x64-2.165.0.tar.gz rm -rf $HOME /Downloads cd $HOME /myagent sudo ./bin/installdependencies.sh ./config.sh You'll see something like this and you are prompted to enter the Azure Devops Organsation ULR and a PAT token: Tip There's an even more slick way to do the above steps here: github.com/MicrosoftDocs/mslearn-azure-pipelines-build-agent/blob/master/build-agent.sh Step 4. Run as a Service \u00b6 Now we usually want to run the agent as a service (i.e. have it run continuously waiting for job to be scheduled) so we need to do the following, as documented here . sudo $HOME /myagent/svc.sh install sudo $HOME /myagent/svc.sh start Installing Capabilities \u00b6 Python Tools \u00b6 The following is the sterling work of my colleague @chyuibacca . The aim was to install the required Python tools onto the agent so that we could run a Pipeline that includes the following Tasks: UsePythonVersion@0 PipAuthenticate@1 As well as a number of script tasks that use the Azure Machine Learning SDK for Python - also known as azureml-sdk (which gets installed via PyPI from pypi.org/project/azureml-defaults/ ) 1. Create Tool Cache Directory \u00b6 mkdir -p $HOME /myagent/_work/_tool 2. Install Python \u00b6 Install tools required to build Python from source: sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev Install required Python version (replace x.y.z with the required Python version, e.g. 3.6.7): wget https://www.python.org/ftp/python/3.6.7/Python-x.y.z.tgz tar -xzf Python-x.y.z.tgz cd Python-x.y.z mkdir -p $HOME /myagent/_work/_tool/Python/x.y.z/x64 ./configure --prefix = $HOME /myagent/_work/_tool/Python/x.y.z/x64/ --enable-optimizations --with-ensurepip = install make -j 8 sudo make altinstall touch $HOME /myagent/_work/_tool/Python/x.y.z/x64.complete If required, create major/ major.minor version link to latest Python version: ln -s $HOME /myagent/_work/_tool/Python/x/x64 $HOME /myagent/_work/_tool/Python/x.y.z/x64 ln -s $HOME /myagent/_work/_tool/Python/x.y/x64 $HOME /myagent/_work/_tool/Python/x.y.z/x64 3. Install Conda \u00b6 Install the latest Conda release: mkdir -p $HOME /myagent/_work/_tool/miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash ./Miniconda3-latest-Linux-x86_64.sh Note: When prompted, select the installation directory as /home/agentsvc/myagent/_work/_tool/minconda Initialise Conda: eval \" $( /home/agentsvc/myagent/_work/_tool/miniconda3/bin/conda shell.bash hook ) \" conda init bash conda config --set auto_activate_base false Make Conda available to the agent by adding /home/agentsvc/myagent/_work/_tool/miniconda3/condabin to the PATH agent environment variable: vi $HOME/myagent/.path","title":"Azure DevOps - Self-Hosted Build Agents"},{"location":"devops/azdo-self-hosted-build-agents/#self-hosted-agents","text":"About Build Agents This new Learning Module has some very good explanations: A build agent is a system that performs build tasks. Think of it as a dedicated server that runs your build process. Imagine that you have an Azure Pipelines project that receives build requests many times per day. Or perhaps you have multiple projects that can each use the same type of build agent. You can organize build agents into agent pools to help ensure that there's a server ready to process each build request. When a build is triggered, Azure Pipelines selects an available build agent from the pool. If all agents are busy, the process waits for one to become available. from docs.microsoft.com/en-us/learn/modules/host-build-agent/ When my colleagues needed a build agent that could communicate with Azure Resources (such as Storage Accounts) inside of a private Virtual Network then the only way to do this without whitelisting a very large set of public IP address ranges (i.e. Azure Datacenter addresses) was to have a Virtual Machine running inside the same private network to which Azure DevOps would schedule build jobs. Azure DevOps provides a way for you to run your own Build Agents called Self-hosted Agents .","title":"Self-Hosted Agents"},{"location":"devops/azdo-self-hosted-build-agents/#configuring-an-agent-on-ubuntu-server","text":"When I learned that the requirement was to build Python wheels to be deployed. I guessed that a Linux Virtual Machine should suffice and be a bit less costly to run that a Windows Server Virtual Machine. I elected to deploy a Ubuntu Server. When I looked into this I found a few blog posts in addition to the official documentation , but all the articles specified using a desktop browser to download the Agent configuration script, and this seemed very odd for Linux where I'd assumed the default mode would be to do everything at the command line so I thought I'd post here how I acheived all of this with just an SSH terminal.","title":"Configuring an agent on Ubuntu Server"},{"location":"devops/azdo-self-hosted-build-agents/#step-0-pre-requisites","text":"Update the apt package manager cache sudo apt-get update","title":"Step 0. Pre-requisites"},{"location":"devops/azdo-self-hosted-build-agents/#step-1-create-an-account-under-which-to-run-the-agent","text":"sudo adduser agentsvc sudo usermod -aG sudo agentsvc # test it su - agentsvc sudo ls -la /root","title":"Step 1. Create an account under which to run the Agent"},{"location":"devops/azdo-self-hosted-build-agents/#step-2-install-git2","text":"sudo add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install git","title":"Step 2. Install Git2"},{"location":"devops/azdo-self-hosted-build-agents/#step-3-install-and-configure-the-agent","text":"mkdir -p $HOME /Downloads && cd $HOME /Downloads curl -o $HOME /Downloads/vsts-agent-linux-x64-2.165.0.tar.gz https://vstsagentpackage.azureedge.net/agent/2.165.0/vsts-agent-linux-x64-2.165.0.tar.gz cd $HOME mkdir -p $HOME /myagent/ && cd $HOME /myagent/ tar zxvf $HOME /Downloads/vsts-agent-linux-x64-2.165.0.tar.gz rm -rf $HOME /Downloads cd $HOME /myagent sudo ./bin/installdependencies.sh ./config.sh You'll see something like this and you are prompted to enter the Azure Devops Organsation ULR and a PAT token: Tip There's an even more slick way to do the above steps here: github.com/MicrosoftDocs/mslearn-azure-pipelines-build-agent/blob/master/build-agent.sh","title":"Step 3. Install and configure the Agent"},{"location":"devops/azdo-self-hosted-build-agents/#step-4-run-as-a-service","text":"Now we usually want to run the agent as a service (i.e. have it run continuously waiting for job to be scheduled) so we need to do the following, as documented here . sudo $HOME /myagent/svc.sh install sudo $HOME /myagent/svc.sh start","title":"Step 4. Run as a Service"},{"location":"devops/azdo-self-hosted-build-agents/#installing-capabilities","text":"","title":"Installing Capabilities"},{"location":"devops/azdo-self-hosted-build-agents/#python-tools","text":"The following is the sterling work of my colleague @chyuibacca . The aim was to install the required Python tools onto the agent so that we could run a Pipeline that includes the following Tasks: UsePythonVersion@0 PipAuthenticate@1 As well as a number of script tasks that use the Azure Machine Learning SDK for Python - also known as azureml-sdk (which gets installed via PyPI from pypi.org/project/azureml-defaults/ )","title":"Python Tools"},{"location":"devops/azdo-self-hosted-build-agents/#1-create-tool-cache-directory","text":"mkdir -p $HOME /myagent/_work/_tool","title":"1. Create Tool Cache Directory"},{"location":"devops/azdo-self-hosted-build-agents/#2-install-python","text":"Install tools required to build Python from source: sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev Install required Python version (replace x.y.z with the required Python version, e.g. 3.6.7): wget https://www.python.org/ftp/python/3.6.7/Python-x.y.z.tgz tar -xzf Python-x.y.z.tgz cd Python-x.y.z mkdir -p $HOME /myagent/_work/_tool/Python/x.y.z/x64 ./configure --prefix = $HOME /myagent/_work/_tool/Python/x.y.z/x64/ --enable-optimizations --with-ensurepip = install make -j 8 sudo make altinstall touch $HOME /myagent/_work/_tool/Python/x.y.z/x64.complete If required, create major/ major.minor version link to latest Python version: ln -s $HOME /myagent/_work/_tool/Python/x/x64 $HOME /myagent/_work/_tool/Python/x.y.z/x64 ln -s $HOME /myagent/_work/_tool/Python/x.y/x64 $HOME /myagent/_work/_tool/Python/x.y.z/x64","title":"2. Install Python"},{"location":"devops/azdo-self-hosted-build-agents/#3-install-conda","text":"Install the latest Conda release: mkdir -p $HOME /myagent/_work/_tool/miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash ./Miniconda3-latest-Linux-x86_64.sh Note: When prompted, select the installation directory as /home/agentsvc/myagent/_work/_tool/minconda Initialise Conda: eval \" $( /home/agentsvc/myagent/_work/_tool/miniconda3/bin/conda shell.bash hook ) \" conda init bash conda config --set auto_activate_base false Make Conda available to the agent by adding /home/agentsvc/myagent/_work/_tool/miniconda3/condabin to the PATH agent environment variable: vi $HOME/myagent/.path","title":"3. Install Conda"},{"location":"devops/devops-for-the-terrified/","text":"Photo by Simon Migaj on Unsplash I've been preparing some videos and learning materials for colleagues that work in Cloud but don't come from a Software Engineering background where disciplines such as version control and continuous integration are taken as granted. Preparing your machine \u00b6 Install the most common extensions you might need. This is using a tool called Scoop which I've talked about before . Open Powershell and run this: Set-ExecutionPolicy RemoteSigned -scope CurrentUser Invoke-Expression ( New-Object System . Net . WebClient ). DownloadString ( 'https://get.scoop.sh' ) scoop bucket add extras scoop install sudo aria2 scoop install git posh-git scoop install vscode windows-terminal Install extensions for VSCode \u00b6 These are the extensions you'll probably need, you can add any VSCode tends to prompt you for any others you may need when you work with a new file type that it recognises is associated with an one or more suggested extensions code - -install-extension ms-azure-devops . azure-pipelines code - -install-extension ms-vscode . azure-account code - -install-extension ms-vscode . azurecli code - -install-extension ms-vscode . powershell code - -install-extension msazurermtools . azurerm-vscode-tools Use Git for Version Control \u00b6 Set some default configuration \u00b6 git config - -system core . longpaths true And enable long paths (see: Enable Long Paths in Windows ) Tell Git who you are (used as the author name/email when you issue the git commit command): Global Default # default details used for all repositories (if not over-ridden by a local config) git config - -global user . name 'Your Name' git config - -global user . email 'your.name@your.org' Current Repository # details used for the current repository (i.e over-riding any global config) git config - -local user . name 'Your Name' git config - -local user . email 'your.name@your.org' What if I forget? If you forget to set your email you get a pretty helpful error message like this: Author identity unknown *** Please tell me who you are. Run git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" to set your account's default identity. Omit --global to set the identity only in this repository. fatal: unable to auto-detect email address Start work on an existing Repository \u00b6 Get the Git Clone URL from a repository you want to work on: Open VSCode then in VS Code Ctrl-Shift-P and then type 'git clone' Change directory into the new directory that gets created after you have cloned the Repository Using different repositories with different credentials If you commit to different repositories with different credentials then you will need to give Git some configuration info about yourself as explained above git config - -local user . name 'Your Name' git config - -local user . email 'your.name@your.org' Create a branch to work on \u00b6 Use the branch command to create the branch and checkout to swap to that branch. git branch feature / TryingSomething git checkout feature / TryingSomething Once you have edited or added some files you need to push you changes (but it's important to update your branch with changes in the trunk to avoid later merge conflicts when making a Pull Request]): git add . git commit -m 'my commit message' # merge the latest changes from main into the feature branch git pull origin main git push - -set-upstream origin feature / TryingSomething Why not just use git push ? i.e. what's the --set-upstream about? You'll get an error if you just use git push because the remote branch doesn't yet exist ): fatal : The current branch feature / TryingSomething has no upstream branch . To push the current branch and set the remote as upstream , use git push - -set-upstream origin feature / TryingSomething Create and merge a Pull Request \u00b6 Open up your Repository in Azure DevOps or GitHub and create a pull request: If all goes well your Pull Request will get approved and merged and you no longer need your local branch for this feature (the remote branch will have been deleted and you need to clean up your working directory). Deleting your branch \u00b6 You no longer need the Feature Branch as it's been deleted on the remote git checkout main ## update the local copy of the main branch git pull # delete the local copy git branch -D feature / TryingSomething Managing changes and conflicts \u00b6 Finding that someone else has modified the same file in a previous (or even concurrent) commit or in another branch that you're now merging your code with can be a truly terror-inducing prospect. I recently discovered how to use Visual Studio Code as the user interface for seeing what the conflict is and deciding whether to take their changes, keep my changes or take some combination of the two - all in the same code editor rather than learning lots of commands or jumping to another tool. I highly recommend reading this post - How to use VS Code as your Git editor, difftool, and mergetool . I couldn't have explained it better myself! Going even faster with git town \u00b6 Clearly this is a lot of commands to have to exectue each time you need to do some Feature work so there's a great tool that makes this much easier! A quicker way to run through the steps starting from create a branch above is to use a tool called Git Town # installs the tool scoop install git-town # makes a feature branch to work on and drops you into it! git town hack feature / TrySomethingElse Do some work... Add-Content newfile . txt '' Add and commit the changes: git add . git commit -m 'Your commit message' This command then does all the hard work of getting your main up to date and merging your local branch and then pushing your work up to the remote (all in one line!) git town sync Create and merge a Pull Request as above # cleans up and deletes redundant local branches git town prune-branches # get everything up to date git town sync Here's the whole process in action!: Creating and completing Pull Requests in Azure Repos from command-line \u00b6 If you're using Azure Repos (Azure DevOps) rather than GitHub then here's a neat way to create a Pull Request (using the Azure CLI DevOps extension ) from the current feature branch without leaving the command prompt Create a Pull request: $prId=(az repos pr create --query 'pullRequestId' -o tsv) Open in browser (NB: we can also use the --open flag with the az repos create command above): az repos pr show --id $prId --open Get details: az repos pr show --id $prId --query '{Id:pullRequestId,Status:status,Title:title}' -o table Set to Approved: az repos pr set-vote --vote approve --id $prId Complete the pull request: az repos pr update --status completed --id $prId --squash true Another way to clean up redundant local branches \u00b6 I use this great tool from Maks Nemisj to remove local branches which are no longer present in my Git remote (usually because closing my Pull Request has also caused the remote branch to be deleted). git removed-branches --prune --force Some common questions \u00b6 I hope to come back and answer these questions in due course: Why are you making me type commands. Can't I just use the GUI? \u00b6 What if I already made a Pull Request and want to make some more changes? \u00b6 What about creating a new Repository from scratch? \u00b6 What if I already have some files locally that I need to put into an existing Repository? \u00b6 How does Git get my remote (i.e. GitHub / Azure DevOps) credentials? \u00b6 Short answer - it gets it from Windows. What if I'm behind a corporate proxy \u00b6 What about deployment pipelines? \u00b6 I'm hoping to write about those too in due course!","title":"DevOps for the Terrified"},{"location":"devops/devops-for-the-terrified/#preparing-your-machine","text":"Install the most common extensions you might need. This is using a tool called Scoop which I've talked about before . Open Powershell and run this: Set-ExecutionPolicy RemoteSigned -scope CurrentUser Invoke-Expression ( New-Object System . Net . WebClient ). DownloadString ( 'https://get.scoop.sh' ) scoop bucket add extras scoop install sudo aria2 scoop install git posh-git scoop install vscode windows-terminal","title":"Preparing your machine"},{"location":"devops/devops-for-the-terrified/#install-extensions-for-vscode","text":"These are the extensions you'll probably need, you can add any VSCode tends to prompt you for any others you may need when you work with a new file type that it recognises is associated with an one or more suggested extensions code - -install-extension ms-azure-devops . azure-pipelines code - -install-extension ms-vscode . azure-account code - -install-extension ms-vscode . azurecli code - -install-extension ms-vscode . powershell code - -install-extension msazurermtools . azurerm-vscode-tools","title":"Install extensions for VSCode"},{"location":"devops/devops-for-the-terrified/#use-git-for-version-control","text":"","title":"Use Git for Version Control"},{"location":"devops/devops-for-the-terrified/#set-some-default-configuration","text":"git config - -system core . longpaths true And enable long paths (see: Enable Long Paths in Windows ) Tell Git who you are (used as the author name/email when you issue the git commit command): Global Default # default details used for all repositories (if not over-ridden by a local config) git config - -global user . name 'Your Name' git config - -global user . email 'your.name@your.org' Current Repository # details used for the current repository (i.e over-riding any global config) git config - -local user . name 'Your Name' git config - -local user . email 'your.name@your.org' What if I forget? If you forget to set your email you get a pretty helpful error message like this: Author identity unknown *** Please tell me who you are. Run git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" to set your account's default identity. Omit --global to set the identity only in this repository. fatal: unable to auto-detect email address","title":"Set some default configuration"},{"location":"devops/devops-for-the-terrified/#start-work-on-an-existing-repository","text":"Get the Git Clone URL from a repository you want to work on: Open VSCode then in VS Code Ctrl-Shift-P and then type 'git clone' Change directory into the new directory that gets created after you have cloned the Repository Using different repositories with different credentials If you commit to different repositories with different credentials then you will need to give Git some configuration info about yourself as explained above git config - -local user . name 'Your Name' git config - -local user . email 'your.name@your.org'","title":"Start work on an existing Repository"},{"location":"devops/devops-for-the-terrified/#create-a-branch-to-work-on","text":"Use the branch command to create the branch and checkout to swap to that branch. git branch feature / TryingSomething git checkout feature / TryingSomething Once you have edited or added some files you need to push you changes (but it's important to update your branch with changes in the trunk to avoid later merge conflicts when making a Pull Request]): git add . git commit -m 'my commit message' # merge the latest changes from main into the feature branch git pull origin main git push - -set-upstream origin feature / TryingSomething Why not just use git push ? i.e. what's the --set-upstream about? You'll get an error if you just use git push because the remote branch doesn't yet exist ): fatal : The current branch feature / TryingSomething has no upstream branch . To push the current branch and set the remote as upstream , use git push - -set-upstream origin feature / TryingSomething","title":"Create a branch to work on"},{"location":"devops/devops-for-the-terrified/#create-and-merge-a-pull-request","text":"Open up your Repository in Azure DevOps or GitHub and create a pull request: If all goes well your Pull Request will get approved and merged and you no longer need your local branch for this feature (the remote branch will have been deleted and you need to clean up your working directory).","title":"Create and merge a Pull Request"},{"location":"devops/devops-for-the-terrified/#deleting-your-branch","text":"You no longer need the Feature Branch as it's been deleted on the remote git checkout main ## update the local copy of the main branch git pull # delete the local copy git branch -D feature / TryingSomething","title":"Deleting your branch"},{"location":"devops/devops-for-the-terrified/#managing-changes-and-conflicts","text":"Finding that someone else has modified the same file in a previous (or even concurrent) commit or in another branch that you're now merging your code with can be a truly terror-inducing prospect. I recently discovered how to use Visual Studio Code as the user interface for seeing what the conflict is and deciding whether to take their changes, keep my changes or take some combination of the two - all in the same code editor rather than learning lots of commands or jumping to another tool. I highly recommend reading this post - How to use VS Code as your Git editor, difftool, and mergetool . I couldn't have explained it better myself!","title":"Managing changes and conflicts"},{"location":"devops/devops-for-the-terrified/#going-even-faster-with-git-town","text":"Clearly this is a lot of commands to have to exectue each time you need to do some Feature work so there's a great tool that makes this much easier! A quicker way to run through the steps starting from create a branch above is to use a tool called Git Town # installs the tool scoop install git-town # makes a feature branch to work on and drops you into it! git town hack feature / TrySomethingElse Do some work... Add-Content newfile . txt '' Add and commit the changes: git add . git commit -m 'Your commit message' This command then does all the hard work of getting your main up to date and merging your local branch and then pushing your work up to the remote (all in one line!) git town sync Create and merge a Pull Request as above # cleans up and deletes redundant local branches git town prune-branches # get everything up to date git town sync Here's the whole process in action!:","title":"Going even faster with git town"},{"location":"devops/devops-for-the-terrified/#creating-and-completing-pull-requests-in-azure-repos-from-command-line","text":"If you're using Azure Repos (Azure DevOps) rather than GitHub then here's a neat way to create a Pull Request (using the Azure CLI DevOps extension ) from the current feature branch without leaving the command prompt Create a Pull request: $prId=(az repos pr create --query 'pullRequestId' -o tsv) Open in browser (NB: we can also use the --open flag with the az repos create command above): az repos pr show --id $prId --open Get details: az repos pr show --id $prId --query '{Id:pullRequestId,Status:status,Title:title}' -o table Set to Approved: az repos pr set-vote --vote approve --id $prId Complete the pull request: az repos pr update --status completed --id $prId --squash true","title":"Creating and completing Pull Requests in Azure Repos from command-line"},{"location":"devops/devops-for-the-terrified/#another-way-to-clean-up-redundant-local-branches","text":"I use this great tool from Maks Nemisj to remove local branches which are no longer present in my Git remote (usually because closing my Pull Request has also caused the remote branch to be deleted). git removed-branches --prune --force","title":"Another way to clean up redundant local branches"},{"location":"devops/devops-for-the-terrified/#some-common-questions","text":"I hope to come back and answer these questions in due course:","title":"Some common questions"},{"location":"devops/devops-for-the-terrified/#why-are-you-making-me-type-commands-cant-i-just-use-the-gui","text":"","title":"Why are you making me type commands. Can't I just use the GUI?"},{"location":"devops/devops-for-the-terrified/#what-if-i-already-made-a-pull-request-and-want-to-make-some-more-changes","text":"","title":"What if I already made a Pull Request and want to make some more changes?"},{"location":"devops/devops-for-the-terrified/#what-about-creating-a-new-repository-from-scratch","text":"","title":"What about creating a new Repository from scratch?"},{"location":"devops/devops-for-the-terrified/#what-if-i-already-have-some-files-locally-that-i-need-to-put-into-an-existing-repository","text":"","title":"What if I already have some files locally that I need to put into an existing Repository?"},{"location":"devops/devops-for-the-terrified/#how-does-git-get-my-remote-ie-github-azure-devops-credentials","text":"Short answer - it gets it from Windows.","title":"How does Git get my remote (i.e. GitHub / Azure DevOps) credentials?"},{"location":"devops/devops-for-the-terrified/#what-if-im-behind-a-corporate-proxy","text":"","title":"What if I'm behind a corporate proxy"},{"location":"devops/devops-for-the-terrified/#what-about-deployment-pipelines","text":"I'm hoping to write about those too in due course!","title":"What about deployment pipelines?"},{"location":"misc/cloud-dev-vms/","text":"This page (probably evolving) describes how I configure myself a Dev VM in Microsoft Azure How I build my workstations in the sky \u00b6 Choosing a Virtual Machine Size in Azure \u00b6 The critical first step in configuring an effective Development environment on the Azure Cloud platform is to get a machine with an appropriate level of performance without incurring too much cost. This page explains sizes for Linux virtual machines in Azure and for Windows . Do you I need nested virtualisation? \u00b6 If I want to use WSL Windows Subsystem for Linux then (as I discovered from this Q&A ) the Virtual Machine size needs to be sufficient to allow nested virtualisation (which is enabled as described in this article ). In other words I need to pick an ACU (Azure Compute Unit) where the processors (CPUs) in a hyper-threaded configuration and therefore capable of running nested virtualization. at the time of writing this includes D_v3 or Ds_v3 , Dv4 or Dsv4 and so on). For a Windows 10 Pro 2004 VM in West Europe I had to pick D4_v3 Linux VM or Linux on a Windows VM? \u00b6 As I mentioned on my Bash shell on Windows article in April I'm still primarily a Windows user but have been gradually using more and more Linux tools and shells and even SSH-ing onto Linux servers for work. As such I've recently decided to spend more time doing my work in a WSL environment on Windows (which allows you to use Linux distros directly from your Windows 10 machine). Now that the next interation of WSL has been released (WSL2) it seemed like a great time to take the next leap. Tips for a great Dev environment with WSL2 \u00b6 What is WSL2? WSL 2 uses an entirely new architecture that uses a real Linux kernel on Windows!. WSL 2 uses the latest and greatest in virtualization technology to run a Linux kernel inside of a lightweight utility virtual machine (VM). WSL 2 is a new version of the architecture that powers the Windows Subsystem for Linux to run ELF64 Linux binaries on Windows. Its primary goals are to increase file system performance, as well as adding full system call compatibility. This new architecture changes how these Linux binaries interact with Windows and your computer\u2019s hardware, but still provides the same user experience as in WSL 1 (the current widely available version). Individual Linux distros can be run either as a WSL 1 distro, or as a WSL 2 distro, can be upgraded or downgraded at any time, and you can run WSL 1 and WSL 2 distros side by side. Get WSL2 \u00b6 Before installing any Linux distributions on Windows, you must enable the \"Windows Subsystem for Linux\" optional feature and for WSL2 you enable the 'Virtual Machine Platform' optional component. Full details are in the links below but be aware that the steps are slightly different when running Windows 10 or Windows Server. If I pick a Windows 10 rather than a Windows Server VM the commands for installing Hyper-V on Windows 10 are slightly different: # Windows 10 (version 2004 and later) Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All # Windows Server 2019 (version 1709 and later) Install-WindowsFeature -Name Hyper-V -ComputerName < computer_name > -IncludeManagementTools Install WSL2 on Windows Server Install WSL2 on Windows 10 Use a single Git Credential Manager on Windows \u00b6 Even thogh you may not be aware of it, when you use Git on Windows Git uses a credential manager component to store authentication tokens securely in Windows Credential Manager. After the first time you enter credentials or an access token Git will to talk to your hosting provider you then the credential manager will ensure that you no longer need to keep re-authenticating. It will just access the token in the Windows Credential Manager. To set up Git Credential Manager for use with a WSL distribution, open your distribution and enter this command (as described in this article ): git config --global credential.helper \"/mnt/c/Program\\ Files/Git/mingw64/libexec/git-core/git-credential-manager.exe\" However, because I use Scoop to install most things, I found that the correct path is as follows: git config --global credential.helper \"/mnt/c/Users/rohanc/scoop/apps/git/current/mingw64/libexec/git-core/git-credential-manager.exe\" Use Visual Studio Code inside of WSL \u00b6 I was amazed to find that I can open VSCode from a directory in the WSL shell (e.g. perhaps I just cloned a code repo and navigated into a folder). I can simply hit code . and I'm off! More here: docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-vscode#open-a-wsl-project-in-visual-studio-code When I open a terminal window in VSCode I'm immediately dropped into the ZSH shell I've got set up in my WSL distro (Ubuntu 18.04 LTS in my case). More tips for a great Dev Environment \u00b6 Much inspiration for this post (using WSL2 and setting up ZSH and OhMyZsh ) was gleaned from this excellent guide: WSL2: Making Windows 10 the perfect dev machine! See also: How to Use Zsh (or Another Shell) in Windows 10","title":"A Dev VM in the sky"},{"location":"misc/cloud-dev-vms/#how-i-build-my-workstations-in-the-sky","text":"","title":"How I build my workstations in the sky"},{"location":"misc/cloud-dev-vms/#choosing-a-virtual-machine-size-in-azure","text":"The critical first step in configuring an effective Development environment on the Azure Cloud platform is to get a machine with an appropriate level of performance without incurring too much cost. This page explains sizes for Linux virtual machines in Azure and for Windows .","title":"Choosing a Virtual Machine Size in Azure"},{"location":"misc/cloud-dev-vms/#do-you-i-need-nested-virtualisation","text":"If I want to use WSL Windows Subsystem for Linux then (as I discovered from this Q&A ) the Virtual Machine size needs to be sufficient to allow nested virtualisation (which is enabled as described in this article ). In other words I need to pick an ACU (Azure Compute Unit) where the processors (CPUs) in a hyper-threaded configuration and therefore capable of running nested virtualization. at the time of writing this includes D_v3 or Ds_v3 , Dv4 or Dsv4 and so on). For a Windows 10 Pro 2004 VM in West Europe I had to pick D4_v3","title":"Do you I need nested virtualisation?"},{"location":"misc/cloud-dev-vms/#linux-vm-or-linux-on-a-windows-vm","text":"As I mentioned on my Bash shell on Windows article in April I'm still primarily a Windows user but have been gradually using more and more Linux tools and shells and even SSH-ing onto Linux servers for work. As such I've recently decided to spend more time doing my work in a WSL environment on Windows (which allows you to use Linux distros directly from your Windows 10 machine). Now that the next interation of WSL has been released (WSL2) it seemed like a great time to take the next leap.","title":"Linux VM or Linux on a Windows VM?"},{"location":"misc/cloud-dev-vms/#tips-for-a-great-dev-environment-with-wsl2","text":"What is WSL2? WSL 2 uses an entirely new architecture that uses a real Linux kernel on Windows!. WSL 2 uses the latest and greatest in virtualization technology to run a Linux kernel inside of a lightweight utility virtual machine (VM). WSL 2 is a new version of the architecture that powers the Windows Subsystem for Linux to run ELF64 Linux binaries on Windows. Its primary goals are to increase file system performance, as well as adding full system call compatibility. This new architecture changes how these Linux binaries interact with Windows and your computer\u2019s hardware, but still provides the same user experience as in WSL 1 (the current widely available version). Individual Linux distros can be run either as a WSL 1 distro, or as a WSL 2 distro, can be upgraded or downgraded at any time, and you can run WSL 1 and WSL 2 distros side by side.","title":"Tips for a great Dev environment with WSL2"},{"location":"misc/cloud-dev-vms/#get-wsl2","text":"Before installing any Linux distributions on Windows, you must enable the \"Windows Subsystem for Linux\" optional feature and for WSL2 you enable the 'Virtual Machine Platform' optional component. Full details are in the links below but be aware that the steps are slightly different when running Windows 10 or Windows Server. If I pick a Windows 10 rather than a Windows Server VM the commands for installing Hyper-V on Windows 10 are slightly different: # Windows 10 (version 2004 and later) Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All # Windows Server 2019 (version 1709 and later) Install-WindowsFeature -Name Hyper-V -ComputerName < computer_name > -IncludeManagementTools Install WSL2 on Windows Server Install WSL2 on Windows 10","title":"Get WSL2"},{"location":"misc/cloud-dev-vms/#use-a-single-git-credential-manager-on-windows","text":"Even thogh you may not be aware of it, when you use Git on Windows Git uses a credential manager component to store authentication tokens securely in Windows Credential Manager. After the first time you enter credentials or an access token Git will to talk to your hosting provider you then the credential manager will ensure that you no longer need to keep re-authenticating. It will just access the token in the Windows Credential Manager. To set up Git Credential Manager for use with a WSL distribution, open your distribution and enter this command (as described in this article ): git config --global credential.helper \"/mnt/c/Program\\ Files/Git/mingw64/libexec/git-core/git-credential-manager.exe\" However, because I use Scoop to install most things, I found that the correct path is as follows: git config --global credential.helper \"/mnt/c/Users/rohanc/scoop/apps/git/current/mingw64/libexec/git-core/git-credential-manager.exe\"","title":"Use a single Git Credential Manager on Windows"},{"location":"misc/cloud-dev-vms/#use-visual-studio-code-inside-of-wsl","text":"I was amazed to find that I can open VSCode from a directory in the WSL shell (e.g. perhaps I just cloned a code repo and navigated into a folder). I can simply hit code . and I'm off! More here: docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-vscode#open-a-wsl-project-in-visual-studio-code When I open a terminal window in VSCode I'm immediately dropped into the ZSH shell I've got set up in my WSL distro (Ubuntu 18.04 LTS in my case).","title":"Use Visual Studio Code inside of WSL"},{"location":"misc/cloud-dev-vms/#more-tips-for-a-great-dev-environment","text":"Much inspiration for this post (using WSL2 and setting up ZSH and OhMyZsh ) was gleaned from this excellent guide: WSL2: Making Windows 10 the perfect dev machine! See also: How to Use Zsh (or Another Shell) in Windows 10","title":"More tips for a great Dev Environment"},{"location":"misc/git-bash/","text":"Using a Bash shell on Windows \u00b6 I'm primarily a Windows 10 user and I primarily use PowerShell as my day-to-day command shell ( Powershell Core 6.2.4 at time of writing ). When I'm reading or editing code or scripts I primarily use Visual Studio Code, so I often use the integrated terminal there, and so my daily default is PowerShell Core there too. For work unrelated to code or for more serious admin work I also tend to have a Windows Terminal open all day too. Even though I have Git for Windows (i.e. scoop install git ) and therefore Git Bash is ready and waiting as the most obvious tool for using Git, I don't generally use Git Bash. I much prefer doing all my Git version control work in PowerShell with the very capable assistance of the posh-git Module . So make no mistake, I'm not advocating using Git Bash all the time. This article is about how to get a smooth experience for those times when I do find myself working in the Git Bash shell, as I'll explain below. I often find code snippets for automating things in my world (mostly Azure Cloud admin and development) tend to assume that I'm using a Linux shell of some sort and so the commands and scripts are in Bash. Specifically, I do lots of things with tools such as Azure CLI and Kubectl (the CLI for Kubernetes) and the examples for these technologies often default to giving examples or scripts in Bash shell syntax. While these are usually pretty simple to transpose into something that could execute in PowerShell (see below) it's still annoying not to be able to copy-and-paste and execute the commands there and then; or add them to a script file for future use without first translating them to PowerShell. Boring option - Transposing between Bash and PowerShell \u00b6 Bash and PowerShell have some aspects of syntax in common (e.g. when referencing a variable like this: $myVariable ) But there are some other things that differ (e.g. when assigning a variable like this): Powershell $myVariable = 'some text' Bash myVariable = 'some text' ... or when setting an environment variable like this: Powershell SET MY_ENV = 'some text' Bash export MY_ENV = 'some text' Shell Trivia SH is Bourne Shell and so Bash is Born-again Bourne Shell Cool option - Git Bash \u00b6 I already had a nice auto-complete experience with posh-git in PowerShell so I want the experience to be just as good in Git Bash, and the auto-complete for Kubectl is one of my reasons for using Git Bash at all so configuring auto-complete is my first step! Coolness pt.1 - Configuring Auto-Complete \u00b6 First off, I need a .bashrc file, so I needed to make one as it didn't exist on my machine: touch ~/.bashrc What is .bashrc ? According to www.maketecheasier.com/what-is-bashrc/ In order to load your preferences, bash runs the contents of the bashrc file at each launch. This shell script is found in each user\u2019s home directory. It\u2019s used to save and load your terminal preferences and environmental variables. Git \u00b6 Here's how to get auto-complete for commands in Git Bash on Windows. mkdir ~/bash_completion.d curl -o ~/bash_completion.d/git https://raw.githubusercontent.com/git/git/master/contrib/completion/git-completion.bash echo \"source ~/bash_completion.d/git\" >> ~/.bashrc If using PowerShell (as I often do) then the posh-git Module is a great option too. scoop install posh-git What is Git? is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. A version-control system is a tool for tracking changes in source code during software development. It is designed for coordinating work among a group of programmers, but it can be used to track changes in any set of files. kubectl CLI \u00b6 The kubectl completion script for Bash can be generated with the command kubectl completion bash more details here Here's how to get auto-complete for kubectl commands in Git Bash on Windows. kubectl completion bash > ~/bash_completion.d/kubectl echo \"source ~/bash_completion.d/kubectl\" >> ~/.bashrc What is Kubectl? Kubectl is a command line tool for controlling Kubernetes clusters. For Azure Kubernetes Service (AKS) users, the Kubectl CLI tool can be installed using the Azure CLI using the command az aks install-cli Docker \u00b6 There also Bash completion files for Docker client , for Docker CLI and for Docker Compose # Docker CLI: curl -o ~/bash_completion.d/docker https://raw.githubusercontent.com/docker/cli/master/contrib/completion/bash/docker echo \"source ~/bash_completion.d/docker\" >> ~/.bashrc # Docker Machine curl -o ~/bash_completion.d/docker-machine https://raw.githubusercontent.com/docker/machine/v0.16.0/contrib/completion/bash/docker-machine.bash echo \"source ~/bash_completion.d/docker-machine\" >> ~/.bashrc # Docker Compose: curl -o ~/bash_completion.d/docker-compose https://raw.githubusercontent.com/docker/compose/1.25.3/contrib/completion/bash/docker-compose echo \"source ~/bash_completion.d/docker-compose\" >> ~/.bashrc Coolness pt.2 - Using Git Bash as an Integrated Shell in VSCode \u00b6 Git Bash is a 'Linux-like' shell experience without going 'all the way' and dropping into the Windows Subsystem of Linx (WSL) Bash shell. Whilst there are times when I'll use WSL too, there are times when I want to be working 'in Windows' but using a more Bash-like experience. This is where Git Bash come to the rescue. Getting Git Bash Git Bash is installed for you when you install Git for Windows, so if you're already using Git then you've kinda got this for free anyway. To get it use Scoop by running this command: [scoop](scoop.md) install git In Visual Studio Code, you can open an integrated terminal, initially starting at the root of your workspace. This can be convenient as you don't have to switch windows or alter the state of an existing terminal to perform a quick command-line task. from: Integrated Terminal As this StackOverflow answer explains you can change the default integrated terminal by updating the setting terminal.integrated.shell.windows . For now, I set mine to C:/Program Files/Git/bin/bash.exe . Note I probably won't keep this as a permanent change to my default integrated terminal, I've also been using the Shell Launcher extension to open up the various shells that I tend to use in VSCode) Now I can use Ctrl-Shift-' ( i.e. the default Key Binding to workbennch.action.terminal.new ) to open a new integrated terminal which will be running Git Bash and to which I can send commands from text editor windows, like this: I also added a Terminal Key Binding for the F8 key to be bound to workbench.action.terminal.runSelectedText so that whilst editing a shell ( .sh ) file in the editor, I can easily send the selected commands to my active terminal in VSCode. -> TLDR; so what?! \u00b6 I already mentioned that I wouldn't normally use this as my preferred shell for Git. I'll only use it for Git when I happen to be in the Git Bash shell for some other reason; so what are those reasons?. I mentioned that I find this useful for Kubernetes admin work (i.e. kubectl ). But it's also more natural than PowerShell for working with the Docker client. For example, if I want to copy and paste code samples for working with Docker such as the ones in the Dockerfile Best Practices page . I'm not aware of a way to transpose the following command stright into PowerShell terminal: docker build - <<EOF FROM busybox RUN echo \"hello world\" EOF ...but I can do it in Git Bash no problem at all!:","title":"Bash Shell on Windows"},{"location":"misc/git-bash/#using-a-bash-shell-on-windows","text":"I'm primarily a Windows 10 user and I primarily use PowerShell as my day-to-day command shell ( Powershell Core 6.2.4 at time of writing ). When I'm reading or editing code or scripts I primarily use Visual Studio Code, so I often use the integrated terminal there, and so my daily default is PowerShell Core there too. For work unrelated to code or for more serious admin work I also tend to have a Windows Terminal open all day too. Even though I have Git for Windows (i.e. scoop install git ) and therefore Git Bash is ready and waiting as the most obvious tool for using Git, I don't generally use Git Bash. I much prefer doing all my Git version control work in PowerShell with the very capable assistance of the posh-git Module . So make no mistake, I'm not advocating using Git Bash all the time. This article is about how to get a smooth experience for those times when I do find myself working in the Git Bash shell, as I'll explain below. I often find code snippets for automating things in my world (mostly Azure Cloud admin and development) tend to assume that I'm using a Linux shell of some sort and so the commands and scripts are in Bash. Specifically, I do lots of things with tools such as Azure CLI and Kubectl (the CLI for Kubernetes) and the examples for these technologies often default to giving examples or scripts in Bash shell syntax. While these are usually pretty simple to transpose into something that could execute in PowerShell (see below) it's still annoying not to be able to copy-and-paste and execute the commands there and then; or add them to a script file for future use without first translating them to PowerShell.","title":"Using a Bash shell on Windows"},{"location":"misc/git-bash/#boring-option-transposing-between-bash-and-powershell","text":"Bash and PowerShell have some aspects of syntax in common (e.g. when referencing a variable like this: $myVariable ) But there are some other things that differ (e.g. when assigning a variable like this): Powershell $myVariable = 'some text' Bash myVariable = 'some text' ... or when setting an environment variable like this: Powershell SET MY_ENV = 'some text' Bash export MY_ENV = 'some text' Shell Trivia SH is Bourne Shell and so Bash is Born-again Bourne Shell","title":"Boring option - Transposing between Bash and PowerShell"},{"location":"misc/git-bash/#cool-option-git-bash","text":"I already had a nice auto-complete experience with posh-git in PowerShell so I want the experience to be just as good in Git Bash, and the auto-complete for Kubectl is one of my reasons for using Git Bash at all so configuring auto-complete is my first step!","title":"Cool option - Git Bash"},{"location":"misc/git-bash/#coolness-pt1-configuring-auto-complete","text":"First off, I need a .bashrc file, so I needed to make one as it didn't exist on my machine: touch ~/.bashrc What is .bashrc ? According to www.maketecheasier.com/what-is-bashrc/ In order to load your preferences, bash runs the contents of the bashrc file at each launch. This shell script is found in each user\u2019s home directory. It\u2019s used to save and load your terminal preferences and environmental variables.","title":"Coolness pt.1 - Configuring Auto-Complete"},{"location":"misc/git-bash/#git","text":"Here's how to get auto-complete for commands in Git Bash on Windows. mkdir ~/bash_completion.d curl -o ~/bash_completion.d/git https://raw.githubusercontent.com/git/git/master/contrib/completion/git-completion.bash echo \"source ~/bash_completion.d/git\" >> ~/.bashrc If using PowerShell (as I often do) then the posh-git Module is a great option too. scoop install posh-git What is Git? is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. A version-control system is a tool for tracking changes in source code during software development. It is designed for coordinating work among a group of programmers, but it can be used to track changes in any set of files.","title":"Git"},{"location":"misc/git-bash/#kubectl-cli","text":"The kubectl completion script for Bash can be generated with the command kubectl completion bash more details here Here's how to get auto-complete for kubectl commands in Git Bash on Windows. kubectl completion bash > ~/bash_completion.d/kubectl echo \"source ~/bash_completion.d/kubectl\" >> ~/.bashrc What is Kubectl? Kubectl is a command line tool for controlling Kubernetes clusters. For Azure Kubernetes Service (AKS) users, the Kubectl CLI tool can be installed using the Azure CLI using the command az aks install-cli","title":"kubectl CLI"},{"location":"misc/git-bash/#docker","text":"There also Bash completion files for Docker client , for Docker CLI and for Docker Compose # Docker CLI: curl -o ~/bash_completion.d/docker https://raw.githubusercontent.com/docker/cli/master/contrib/completion/bash/docker echo \"source ~/bash_completion.d/docker\" >> ~/.bashrc # Docker Machine curl -o ~/bash_completion.d/docker-machine https://raw.githubusercontent.com/docker/machine/v0.16.0/contrib/completion/bash/docker-machine.bash echo \"source ~/bash_completion.d/docker-machine\" >> ~/.bashrc # Docker Compose: curl -o ~/bash_completion.d/docker-compose https://raw.githubusercontent.com/docker/compose/1.25.3/contrib/completion/bash/docker-compose echo \"source ~/bash_completion.d/docker-compose\" >> ~/.bashrc","title":"Docker"},{"location":"misc/git-bash/#coolness-pt2-using-git-bash-as-an-integrated-shell-in-vscode","text":"Git Bash is a 'Linux-like' shell experience without going 'all the way' and dropping into the Windows Subsystem of Linx (WSL) Bash shell. Whilst there are times when I'll use WSL too, there are times when I want to be working 'in Windows' but using a more Bash-like experience. This is where Git Bash come to the rescue. Getting Git Bash Git Bash is installed for you when you install Git for Windows, so if you're already using Git then you've kinda got this for free anyway. To get it use Scoop by running this command: [scoop](scoop.md) install git In Visual Studio Code, you can open an integrated terminal, initially starting at the root of your workspace. This can be convenient as you don't have to switch windows or alter the state of an existing terminal to perform a quick command-line task. from: Integrated Terminal As this StackOverflow answer explains you can change the default integrated terminal by updating the setting terminal.integrated.shell.windows . For now, I set mine to C:/Program Files/Git/bin/bash.exe . Note I probably won't keep this as a permanent change to my default integrated terminal, I've also been using the Shell Launcher extension to open up the various shells that I tend to use in VSCode) Now I can use Ctrl-Shift-' ( i.e. the default Key Binding to workbennch.action.terminal.new ) to open a new integrated terminal which will be running Git Bash and to which I can send commands from text editor windows, like this: I also added a Terminal Key Binding for the F8 key to be bound to workbench.action.terminal.runSelectedText so that whilst editing a shell ( .sh ) file in the editor, I can easily send the selected commands to my active terminal in VSCode.","title":"Coolness pt.2 - Using Git Bash as an Integrated Shell in VSCode"},{"location":"misc/git-bash/#-tldr-so-what","text":"I already mentioned that I wouldn't normally use this as my preferred shell for Git. I'll only use it for Git when I happen to be in the Git Bash shell for some other reason; so what are those reasons?. I mentioned that I find this useful for Kubernetes admin work (i.e. kubectl ). But it's also more natural than PowerShell for working with the Docker client. For example, if I want to copy and paste code samples for working with Docker such as the ones in the Dockerfile Best Practices page . I'm not aware of a way to transpose the following command stright into PowerShell terminal: docker build - <<EOF FROM busybox RUN echo \"hello world\" EOF ...but I can do it in Git Bash no problem at all!:","title":"-&gt; TLDR; so what?!"},{"location":"misc/misc/","text":"GitHub CLI and Command Completions in Powershell \u00b6 This comment in the CLI issues gave me a hint how best to add command completions for GitHub CLI gh in PowerShell (the shell I still use most often - and the GitHub CLI docs only mention bash and zsh ). In my $profile file I added this line, so to edit my PowerShell profile I execute code $profile and then: Invoke-Expression -Command $( gh completion -s powershell | Out-String ) Windows Subsystem for Linux (WSL) environment \u00b6 Install: docs.microsoft.com/en-us/windows/wsl/install-win10 I found some really handy notes here: jwendl.net/code-notes/wsl/install/ As always, Scott Hanselman has the low-down to end all low-downs: Cool tips & tricks for WSL Shiny Linux Dev Environment How to make a pretty prompt in Windows Terminal Info See my page on Scoop & Co. for how to quickly and easily install Delugia Nerd Font mentioned in Scott's pretty prompt post above Free Resources for Publishing \u00b6 Unsplash The internet\u2019s source of freely usable images. Powered by creators everywhere. Photo by Zoe Schaeffer","title":"Miscellanous Notes"},{"location":"misc/misc/#github-cli-and-command-completions-in-powershell","text":"This comment in the CLI issues gave me a hint how best to add command completions for GitHub CLI gh in PowerShell (the shell I still use most often - and the GitHub CLI docs only mention bash and zsh ). In my $profile file I added this line, so to edit my PowerShell profile I execute code $profile and then: Invoke-Expression -Command $( gh completion -s powershell | Out-String )","title":"GitHub CLI and Command Completions in Powershell"},{"location":"misc/misc/#windows-subsystem-for-linux-wsl-environment","text":"Install: docs.microsoft.com/en-us/windows/wsl/install-win10 I found some really handy notes here: jwendl.net/code-notes/wsl/install/ As always, Scott Hanselman has the low-down to end all low-downs: Cool tips & tricks for WSL Shiny Linux Dev Environment How to make a pretty prompt in Windows Terminal Info See my page on Scoop & Co. for how to quickly and easily install Delugia Nerd Font mentioned in Scott's pretty prompt post above","title":"Windows Subsystem for Linux (WSL) environment"},{"location":"misc/misc/#free-resources-for-publishing","text":"Unsplash The internet\u2019s source of freely usable images. Powered by creators everywhere. Photo by Zoe Schaeffer","title":"Free Resources for Publishing"},{"location":"misc/ps-module-paths/","text":"This page relates my battle with OneDrive folder backup and how I fixed my broken $env:PSModulePath The Problem \u00b6 When trying to use ConditionalAccessAsCode I ran into a problem with running one of it's dependent modules which is the AzureADPreview module (which is only compatible with PowerShell 5.1 and not PowerShell Core (i.e. versions 6 and above)) After running Install-Module -Name AzureADPreview in PowerShell 5.1 I was getting an error running Import-Module -Name AzureADPreview : The specified module was not loaded because no valid module file was found in any module directory Which seemed very strange given that it seemed to have installed without error. After quick DuckDuckGo search I got a clue from this blog article by Salaudeen Rajack I worked out that with Install-Module the module was being installed into: C:\\Users\\rohan\\OneDrive\\Documents\\WindowsPowershell\\Modules ..wheras Import-Module was trying to load modules from: C:\\Users\\rohan\\Documents\\WindowsPowershell\\Modules . I quickly realised that this was because I had enabled the personal folder backup feature in OneDrive and that it had changed the path of my MyDocuments folder. The Solution(s) \u00b6 Update PSModulesPath \u00b6 As I subsequently discovered, it does state in this document on that: Microsoft OneDrive can also change the location of your Documents folder. You can verify the location of your Documents folder using the following command: [Environment]::GetFolderPath('MyDocuments'). So the cause of my problem was that the path of my MyDocuments folder had been updated form C:\\Users\\rohan\\Documents to C:\\Users\\rohan\\OneDrive\\Documents (after enabling OneDrive PC folder backup ), but that the entry for PSModulePath in my [User] Environment Variable (in PowerShell $env:PSModulePath ) had not been updated accordingly. One solution available to me was to update the value of PSModulePath in my [User] Environment Variable to add the \\OneDrive\\ segment into the entry for the MyDocuments folder Disable OneDrive folder backup \u00b6 If I disable backup then the path can go back to normal and will resolve the issue with $env:PSModulePath without having to edit the Environment Variable. See the section 'Manage or stop PC folder backup' in this article As it states: When you stop backing up a folder, the files that were already backed up by OneDrive stay in the OneDrive folder, and will no longer appear in your device folder. And so I also then need to move the Powershell and WindowsPowershell folders back into the MyDocuments folder Change the location of OneDrive folder \u00b6 I'll note this here too as I didn't do this myself and it still won't prevent the issue if you elect to use OneDrive personal folder backup... Change the location of your OneDrive folder A demo of PSModulePath in action \u00b6 PowerShell 5 and PowerShell Core download modules into different paths and so I've prepared a couple of demos below to illustrate this. The full script for the demo is: CurrentUser \"Running in Powershell $( $PSVersionTable . PSVersion . Major ) . $( $PSVersionTable . PSVersion . Minor ) \" $env:PSModulePath Install-Module -Name PSLogging -Scope CurrentUser Get-InstalledModule Get-InstalledModule -Name PSLogging Get-Module -Name PSLogging Import-Module -Name PSLogging Get-Module -Name PSLogging ( Get-Module -Name PSLogging ). Path Remove-Module -Name PSLogging Uninstall-Module -Name PSLogging AllUsers \"Running in Powershell $( $PSVersionTable . PSVersion . Major ) . $( $PSVersionTable . PSVersion . Minor ) \" $env:PSModulePath # Note: Installing sudo to avoid having to open an elevated shell scoop install sudo sudo Install-Module -Name PSLogging -Scope AllUsers Get-InstalledModule Get-InstalledModule -Name PSLogging Get-Module -Name PSLogging Import-Module -Name PSLogging Get-Module -Name PSLogging ( Get-Module -Name PSLogging ). Path Remove-Module -Name PSLogging sudo Uninstall-Module -Name PSLogging Here's a demo in PowerShell 5.1 : ... and here's the same demo in PowerShell 7.1 :","title":"About PowerShell Module Paths"},{"location":"misc/ps-module-paths/#the-problem","text":"When trying to use ConditionalAccessAsCode I ran into a problem with running one of it's dependent modules which is the AzureADPreview module (which is only compatible with PowerShell 5.1 and not PowerShell Core (i.e. versions 6 and above)) After running Install-Module -Name AzureADPreview in PowerShell 5.1 I was getting an error running Import-Module -Name AzureADPreview : The specified module was not loaded because no valid module file was found in any module directory Which seemed very strange given that it seemed to have installed without error. After quick DuckDuckGo search I got a clue from this blog article by Salaudeen Rajack I worked out that with Install-Module the module was being installed into: C:\\Users\\rohan\\OneDrive\\Documents\\WindowsPowershell\\Modules ..wheras Import-Module was trying to load modules from: C:\\Users\\rohan\\Documents\\WindowsPowershell\\Modules . I quickly realised that this was because I had enabled the personal folder backup feature in OneDrive and that it had changed the path of my MyDocuments folder.","title":"The Problem"},{"location":"misc/ps-module-paths/#the-solutions","text":"","title":"The Solution(s)"},{"location":"misc/ps-module-paths/#update-psmodulespath","text":"As I subsequently discovered, it does state in this document on that: Microsoft OneDrive can also change the location of your Documents folder. You can verify the location of your Documents folder using the following command: [Environment]::GetFolderPath('MyDocuments'). So the cause of my problem was that the path of my MyDocuments folder had been updated form C:\\Users\\rohan\\Documents to C:\\Users\\rohan\\OneDrive\\Documents (after enabling OneDrive PC folder backup ), but that the entry for PSModulePath in my [User] Environment Variable (in PowerShell $env:PSModulePath ) had not been updated accordingly. One solution available to me was to update the value of PSModulePath in my [User] Environment Variable to add the \\OneDrive\\ segment into the entry for the MyDocuments folder","title":"Update PSModulesPath"},{"location":"misc/ps-module-paths/#disable-onedrive-folder-backup","text":"If I disable backup then the path can go back to normal and will resolve the issue with $env:PSModulePath without having to edit the Environment Variable. See the section 'Manage or stop PC folder backup' in this article As it states: When you stop backing up a folder, the files that were already backed up by OneDrive stay in the OneDrive folder, and will no longer appear in your device folder. And so I also then need to move the Powershell and WindowsPowershell folders back into the MyDocuments folder","title":"Disable OneDrive folder backup"},{"location":"misc/ps-module-paths/#change-the-location-of-onedrive-folder","text":"I'll note this here too as I didn't do this myself and it still won't prevent the issue if you elect to use OneDrive personal folder backup... Change the location of your OneDrive folder","title":"Change the location of OneDrive folder"},{"location":"misc/ps-module-paths/#a-demo-of-psmodulepath-in-action","text":"PowerShell 5 and PowerShell Core download modules into different paths and so I've prepared a couple of demos below to illustrate this. The full script for the demo is: CurrentUser \"Running in Powershell $( $PSVersionTable . PSVersion . Major ) . $( $PSVersionTable . PSVersion . Minor ) \" $env:PSModulePath Install-Module -Name PSLogging -Scope CurrentUser Get-InstalledModule Get-InstalledModule -Name PSLogging Get-Module -Name PSLogging Import-Module -Name PSLogging Get-Module -Name PSLogging ( Get-Module -Name PSLogging ). Path Remove-Module -Name PSLogging Uninstall-Module -Name PSLogging AllUsers \"Running in Powershell $( $PSVersionTable . PSVersion . Major ) . $( $PSVersionTable . PSVersion . Minor ) \" $env:PSModulePath # Note: Installing sudo to avoid having to open an elevated shell scoop install sudo sudo Install-Module -Name PSLogging -Scope AllUsers Get-InstalledModule Get-InstalledModule -Name PSLogging Get-Module -Name PSLogging Import-Module -Name PSLogging Get-Module -Name PSLogging ( Get-Module -Name PSLogging ). Path Remove-Module -Name PSLogging sudo Uninstall-Module -Name PSLogging Here's a demo in PowerShell 5.1 : ... and here's the same demo in PowerShell 7.1 :","title":"A demo of PSModulePath in action"},{"location":"misc/scoop/","text":"This page describes how I'm current -ly using Scoop (and other Package Managers) to configure my system The Daddy... Scoop ! \u00b6 Get scoop.sh and check out the Scoop Home Page (which seems to have superceded the wiki ) for latest info - or see below for the TL;DR Scoop focuses on open-source, command-line developer tools\" but then those are the kinds of tools I'm using more and more... ...You're familiar with UNIX tools, and you wish there were more of them on Windows from: github.com/lukesampson/scoop/wiki/Chocolatey-Comparison Install scoop and base set of tools \u00b6 Set-ExecutionPolicy RemoteSigned -Scope CurrentUser Invoke-Expression ( New-Object System . Net . WebClient ). DownloadString ( 'https://get.scoop.sh' ) scoop help scoop install 7zip git scoop install sudo scoop install aria2 lessmsi innounp scoop install curl grep sed less dig nano scoop install coreutils # vim scoop install vim ' set ff=unix set cindent set tabstop=4 set shiftwidth=4 set expandtab set backupdir=$TEMP ' | out-file ~/. vimrc -enc oem -append coreutils is a multi-tool package - \"a collection of GNU utilities such as bash, make, gawk and grep based on MSYS \" Tip you can use the Unix tool ls after installing coreutils but you first need to remove the PowerShell alias already in place i.e. add this to your Powershell $profile : scoop install pwsh ...switch to powershell core #powershell 6 and above Remove-Alias -Name ls Remove-Alias -Name cat Remove-Alias -Name mv Remove-Alias -Name ps Remove-Alias -Name pwd Remove-Alias -Name rm Buckets \u00b6 Then, I add additional Buckets . Buckets are collections of apps which are additional / optional to the main bucket scoop bucket add extras scoop bucket add versions scoop bucket add Sysinternals 'https://github.com/Ash258/Scoop-Sysinternals.git' Then yet more handy tools I use ( some are from the **extras* bucket*): scoop install vscode scoop install go docker kubectl helm make scoop install azure-cli azure-ps storageexplorer dotnet-sdk scoop install nodejs yarn openssl scoop install azure-functions-core-tools scoop install vcredist2019 scoop install notepadplusplus windows-terminal postman scoop install paint . net krita brackets scoop install gh scoop bucket add instrumenta https :// github . com / instrumenta / scoop-instrumenta scoop install kubeval scoop install conftest # SysInternals scoop install Autoruns ZoomIt DiskView ProcessExplorer TCPView # Used to install with scoop but are now in PSGallery Install-PackageProvider -Name NuGet -MinimumVersion 2 . 8 . 5 . 201 -Force -Scope CurrentUser Install-Module posh-git -Scope CurrentUser Install-Module oh-my-posh -Scope CurrentUser Info Other useful (possibly useful?) buckets that I've not yet had a use for: nonportable - non-portable Applications that need to retain state between versions full list of known buckets Paths \u00b6 Referencing the path to an application installed by Scoop %UserProfile%/scoop/apps Note Those installed with the --global (and with the sudo command) will reside in the path %ProgramData%/scoop/apps For each version of an application the files will be in a directory with the version number, but Scoop creates a Shim for the current version in the path %UserProfile%\\scoop\\apps\\{AppName}\\current . For example: the path to Python ( python.exe ) will be either: %UserProfile%\\scoop\\apps\\python\\3.8.1\\python.exe or: %UserProfile%\\scoop\\apps\\python\\current\\python.exe For system tools you'll probably want to use the current shim to avoid those tools breaking between updates. Specifying Application Versions \u00b6 The versions bucket contains a way to obtain versions other than the latest version of an application. This is used in combination with scoop reset command to switch between versions of an app. Scoop creates a shim for each version and scoop reset switches the current shim between those versions. For example: Switching-Ruby-And-Python-Versions Updating Applications \u00b6 scoop status is used to display any available updates: ...and then scoop update is used to update one or more applications (in a single command if you like!). Other miscellany \u00b6 Multi-Connection Downloads \u00b6 Downloads can be speeded up by using Aria2 . See also lukesampson/scoop#multi-connection-downloads-with-aria2 MSI extraction \u00b6 If installed, scoop can be prevented from using it if necessary by running scoop config aria2-enabled $false I had a problem installing Brackets and it was resolved by installing LessMSI - but at the moment I'm not sure why... !!!info What is LessMSI? A tool to view and extract the contents of a Windows Installer (.msi) file. scoop install lessmsi scoop config MSIEXTRACT_USE_LESSMSI $true Common Pre-Requisites \u00b6 The following is a set of common pre-requisites for installing tools and utilities (e.g. the pip package manager for python tools): Python and PIP \u00b6 scoop install python miniconda3 scoop install curl curl https :// bootstrap . pypa . io / get-pip . py -o get-pip . py python get-pip . py # pip and other tools be later upgraded by re-running the above or running: python -m pip install -U pip # when running behind a corporate propxy, the following command should still work: sudo pip install - -upgrade - -trusted-host pypi . org - -trusted-host files . pythonhosted . org pip setuptools wheel System Fonts \u00b6 Here's another place where Scoop comes to the rescue to avoid clunky download and installs for system fonts! Info note how sudo is being used to install the font as a global / system font - this obvisouly pops up a UAC prompt as it requires elevated provilege to install a system font... scoop bucket add nerd-fonts sudo scoop install Delugia-Nerd-Font-Complete Cascadia-Code Productivity Tools \u00b6 MkDocs \u00b6 MkDocs \"Project documentation with Markdown\" I use this for writing this site! : pip install mkdocs python .\\ scoop \\ apps \\ python \\ current \\ Tools \\ scripts \\ win_add2path . py Install the Custom Theme \u00b6 Using Material theme and dependencies for CodeHilite pip install mkdocs-material pip install pygments # for source code syntax highlighting PowerSession \u00b6 This a version of asciinema for recording and re-playing PowerShell terminal sessions. Once recorded you upload the recording and share it with the world! What is asciinema and how do I get it?! If you've never used asciinema before and you want to share a demo of something at a terminal then you'll love this tool. Simply go to asciinema.org/ click on Log in / Sign up Enter an email address Click on the link in the confirmation email Choose a username Stay logged into that browser on that machine and you can then PowerSession auth to create a link between the terminal and your asciinema account Apparently tt's based on Windows Pseudo Console (ConPTY). # Installation scoop install PowerSession # Usage: Log in on the machine where you want to make a recording Powersession auth # ... copy the URL you're given and paste it into your browser as instructed # Record a Terminal session (it will open a new session for you) PowerSession rec a . txt # Play back the recording to check that it's OK to upload PowerSession play a . txt # If it's ok then PowerSession upload a . txt An example of an uploaded recording looks like this!","title":"Scoop & Co"},{"location":"misc/scoop/#the-daddy-scoop","text":"Get scoop.sh and check out the Scoop Home Page (which seems to have superceded the wiki ) for latest info - or see below for the TL;DR Scoop focuses on open-source, command-line developer tools\" but then those are the kinds of tools I'm using more and more... ...You're familiar with UNIX tools, and you wish there were more of them on Windows from: github.com/lukesampson/scoop/wiki/Chocolatey-Comparison","title":"The Daddy... Scoop!"},{"location":"misc/scoop/#install-scoop-and-base-set-of-tools","text":"Set-ExecutionPolicy RemoteSigned -Scope CurrentUser Invoke-Expression ( New-Object System . Net . WebClient ). DownloadString ( 'https://get.scoop.sh' ) scoop help scoop install 7zip git scoop install sudo scoop install aria2 lessmsi innounp scoop install curl grep sed less dig nano scoop install coreutils # vim scoop install vim ' set ff=unix set cindent set tabstop=4 set shiftwidth=4 set expandtab set backupdir=$TEMP ' | out-file ~/. vimrc -enc oem -append coreutils is a multi-tool package - \"a collection of GNU utilities such as bash, make, gawk and grep based on MSYS \" Tip you can use the Unix tool ls after installing coreutils but you first need to remove the PowerShell alias already in place i.e. add this to your Powershell $profile : scoop install pwsh ...switch to powershell core #powershell 6 and above Remove-Alias -Name ls Remove-Alias -Name cat Remove-Alias -Name mv Remove-Alias -Name ps Remove-Alias -Name pwd Remove-Alias -Name rm","title":"Install scoop and base set of tools"},{"location":"misc/scoop/#buckets","text":"Then, I add additional Buckets . Buckets are collections of apps which are additional / optional to the main bucket scoop bucket add extras scoop bucket add versions scoop bucket add Sysinternals 'https://github.com/Ash258/Scoop-Sysinternals.git' Then yet more handy tools I use ( some are from the **extras* bucket*): scoop install vscode scoop install go docker kubectl helm make scoop install azure-cli azure-ps storageexplorer dotnet-sdk scoop install nodejs yarn openssl scoop install azure-functions-core-tools scoop install vcredist2019 scoop install notepadplusplus windows-terminal postman scoop install paint . net krita brackets scoop install gh scoop bucket add instrumenta https :// github . com / instrumenta / scoop-instrumenta scoop install kubeval scoop install conftest # SysInternals scoop install Autoruns ZoomIt DiskView ProcessExplorer TCPView # Used to install with scoop but are now in PSGallery Install-PackageProvider -Name NuGet -MinimumVersion 2 . 8 . 5 . 201 -Force -Scope CurrentUser Install-Module posh-git -Scope CurrentUser Install-Module oh-my-posh -Scope CurrentUser Info Other useful (possibly useful?) buckets that I've not yet had a use for: nonportable - non-portable Applications that need to retain state between versions full list of known buckets","title":"Buckets"},{"location":"misc/scoop/#paths","text":"Referencing the path to an application installed by Scoop %UserProfile%/scoop/apps Note Those installed with the --global (and with the sudo command) will reside in the path %ProgramData%/scoop/apps For each version of an application the files will be in a directory with the version number, but Scoop creates a Shim for the current version in the path %UserProfile%\\scoop\\apps\\{AppName}\\current . For example: the path to Python ( python.exe ) will be either: %UserProfile%\\scoop\\apps\\python\\3.8.1\\python.exe or: %UserProfile%\\scoop\\apps\\python\\current\\python.exe For system tools you'll probably want to use the current shim to avoid those tools breaking between updates.","title":"Paths"},{"location":"misc/scoop/#specifying-application-versions","text":"The versions bucket contains a way to obtain versions other than the latest version of an application. This is used in combination with scoop reset command to switch between versions of an app. Scoop creates a shim for each version and scoop reset switches the current shim between those versions. For example: Switching-Ruby-And-Python-Versions","title":"Specifying Application Versions"},{"location":"misc/scoop/#updating-applications","text":"scoop status is used to display any available updates: ...and then scoop update is used to update one or more applications (in a single command if you like!).","title":"Updating Applications"},{"location":"misc/scoop/#other-miscellany","text":"","title":"Other miscellany"},{"location":"misc/scoop/#multi-connection-downloads","text":"Downloads can be speeded up by using Aria2 . See also lukesampson/scoop#multi-connection-downloads-with-aria2","title":"Multi-Connection Downloads"},{"location":"misc/scoop/#msi-extraction","text":"If installed, scoop can be prevented from using it if necessary by running scoop config aria2-enabled $false I had a problem installing Brackets and it was resolved by installing LessMSI - but at the moment I'm not sure why... !!!info What is LessMSI? A tool to view and extract the contents of a Windows Installer (.msi) file. scoop install lessmsi scoop config MSIEXTRACT_USE_LESSMSI $true","title":"MSI extraction"},{"location":"misc/scoop/#common-pre-requisites","text":"The following is a set of common pre-requisites for installing tools and utilities (e.g. the pip package manager for python tools):","title":"Common Pre-Requisites"},{"location":"misc/scoop/#python-and-pip","text":"scoop install python miniconda3 scoop install curl curl https :// bootstrap . pypa . io / get-pip . py -o get-pip . py python get-pip . py # pip and other tools be later upgraded by re-running the above or running: python -m pip install -U pip # when running behind a corporate propxy, the following command should still work: sudo pip install - -upgrade - -trusted-host pypi . org - -trusted-host files . pythonhosted . org pip setuptools wheel","title":"Python and PIP"},{"location":"misc/scoop/#system-fonts","text":"Here's another place where Scoop comes to the rescue to avoid clunky download and installs for system fonts! Info note how sudo is being used to install the font as a global / system font - this obvisouly pops up a UAC prompt as it requires elevated provilege to install a system font... scoop bucket add nerd-fonts sudo scoop install Delugia-Nerd-Font-Complete Cascadia-Code","title":"System Fonts"},{"location":"misc/scoop/#productivity-tools","text":"","title":"Productivity Tools"},{"location":"misc/scoop/#mkdocs","text":"MkDocs \"Project documentation with Markdown\" I use this for writing this site! : pip install mkdocs python .\\ scoop \\ apps \\ python \\ current \\ Tools \\ scripts \\ win_add2path . py","title":"MkDocs"},{"location":"misc/scoop/#install-the-custom-theme","text":"Using Material theme and dependencies for CodeHilite pip install mkdocs-material pip install pygments # for source code syntax highlighting","title":"Install the Custom Theme"},{"location":"misc/scoop/#powersession","text":"This a version of asciinema for recording and re-playing PowerShell terminal sessions. Once recorded you upload the recording and share it with the world! What is asciinema and how do I get it?! If you've never used asciinema before and you want to share a demo of something at a terminal then you'll love this tool. Simply go to asciinema.org/ click on Log in / Sign up Enter an email address Click on the link in the confirmation email Choose a username Stay logged into that browser on that machine and you can then PowerSession auth to create a link between the terminal and your asciinema account Apparently tt's based on Windows Pseudo Console (ConPTY). # Installation scoop install PowerSession # Usage: Log in on the machine where you want to make a recording Powersession auth # ... copy the URL you're given and paste it into your browser as instructed # Record a Terminal session (it will open a new session for you) PowerSession rec a . txt # Play back the recording to check that it's OK to upload PowerSession play a . txt # If it's ok then PowerSession upload a . txt An example of an uploaded recording looks like this!","title":"PowerSession"},{"location":"misc/shapes/","text":"When I need to draw technical diagrams for projects we're working on using Azure I need shapes that represent the services we're using and it's not too difficult to find sets of shapes that can be used in Microsoft Visio. But I don't have a licence for Visio and so instead I tend to use the popular drawing tool Draw.io Incidentally, Draw.io is now called Diagrams.net After a quick search one I found was a Visio stencil for Azure architecture design and documentation. Importing Shapes from a Visio stencil \u00b6 Here's how Download the zip file from Dave Summers' Github Repository : Then Import the .vssx file: Then you'll want to edit the name of the shapes folder like this: Draw-io will download a .drawio file which you can ignore if you wish: Here's how the shapes look when you hover over them: Here's how the shapes will show up in search results when you search for them: Other sources of Shapes \u00b6 There are some other shapes out there too such as the Microsoft Integration Stencils Pack for Visio by Sandro Pereira ( @sandro_asp )","title":"Import Shapes into Draw.io"},{"location":"misc/shapes/#importing-shapes-from-a-visio-stencil","text":"Here's how Download the zip file from Dave Summers' Github Repository : Then Import the .vssx file: Then you'll want to edit the name of the shapes folder like this: Draw-io will download a .drawio file which you can ignore if you wish: Here's how the shapes look when you hover over them: Here's how the shapes will show up in search results when you search for them:","title":"Importing Shapes from a Visio stencil"},{"location":"misc/shapes/#other-sources-of-shapes","text":"There are some other shapes out there too such as the Microsoft Integration Stencils Pack for Visio by Sandro Pereira ( @sandro_asp )","title":"Other sources of Shapes"},{"location":"misc/test-page/","text":"Arithmatex \u00b6 E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i,j}w_{ij}v_i h_j - \\sum_i b_i v_i - \\sum_j c_j h_j E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i,j}w_{ij}v_i h_j - \\sum_i b_i v_i - \\sum_j c_j h_j 3 < 4 3 < 4 \\begin{align} p(v_i=1|\\mathbf{h}) & = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\ p(h_j=1|\\mathbf{v}) & = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right) \\end{align} \\begin{align} p(v_i=1|\\mathbf{h}) & = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\ p(h_j=1|\\mathbf{v}) & = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right) \\end{align} Carets \u00b6 Testing carets Critic \u00b6 Don't go around saying to people that the world owes you a living. The world owes you nothing. It was here first. One Only one thing is impossible for God: To find any sense in any copyright law on the planet. Truth is stranger than fiction strange but true , but it is because Fiction is obliged to stick to possibilities; Truth isn\u2019t. Don't act so surprised, Your Queen Highness @gsw sounds better . You weren't on any mercy mission this time. Details \u00b6 Open styled details Nested details! And more content again. Success Content. Warning Content. Emoji \u00b6 EscapeAll \u00b6 We can escape everything! \u2764\ud83d\ude04 Highlight \u00b6 $a = array ( \"foo\" => 0 , \"bar\" => 1 ); InlineHilite \u00b6 Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . inline math \u00b6 p(x|y) = \\frac{p(y|x)p(x)}{p(y)} Keys \u00b6 Ctrl \uff0b Alt \uff0b Del auto linking \u00b6 Just paste links directly in the document like this: google.com . Or even an email address: fake.email@email.com . mentions \u00b6 GitHub: @aclk GitLab: @aclk BitBucket: @aclk Twitter: @aqingo repo mentions \u00b6 facelessuser/pymdown-extensions pycqa/flake8 Issues and PRs \u00b6 Issue #1 Issue backrefs#1 Issue Python-Markdown/markdown#1 Issue pycqa/flake8#385 Pull request !13 Pull request backrefs!4 Pull request Python-Markdown/markdown!598 Pull request pycqa/flake8!213 commits \u00b6 181c06d backrefs@cb4ecc5 Python-Markdown/markdown@de5c696 pycqa/flake8@8acf55e0 compare \u00b6 e2ed7e0...90b6fb8 backrefs@88c6238...cb4ecc5 Python-Markdown/markdown@007bd2a...de5c696 pycqa/flake8@1ecf9700...9bea7576 link shorteners - external \u00b6 @facelessuser facelessuser/pymdown-extensions pycqa/flake8#385 mrabarnett/mrab-regex#260 internal \u00b6 facelessuser/pymdown-extensions#1 facelessuser/pymdown-extensions!13 facelessuser/pymdown-extensions@3f6b07a facelessuser/pymdown-extensions@e2ed7e0...90b6fb8 facelessuser/Rummage@181c06d Mark \u00b6 mark me smart==mark Progress Bars \u00b6 0% 5% 25% 45% 65% 85% 100% 85% 100% Smart Symbols \u00b6 \u2122 \u00a9 \u00ae \u2105 \u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc, etc. 1 st 2 nd etc. Snippets \u00b6 from snip 1 \u00b6 from snip 1 from snip 2 \u00b6 from snip 2 superfences \u00b6 Injecting Classes import hello_world Non-Pygments Injecting Classes import hello_world Tabbed fences Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main ( void ) { printf ( \"hello, world \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello, world! \\n \" ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello, world!\" ); } } in context ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d import foo.bar import foo.bar \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Sequence Diagram Example sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts <br/>prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! Tabbed \u00b6 Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b two separate tab sets \u00b6 Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b Tab A Different tab set. Tab B More content. Tasklist \u00b6 Task List item 1 item A item B more text item a item b item c item C item 2 item 3 tilde \u00b6 Delete: Delete me Subscript: CH 3 CH 2 OH text a subscript var config = { startOnLoad: false, theme: null, flowchart: { htmlLabels: false }, sequence: { useMaxWidth: false }, class: { textHeight: 16, dividerMargin: 16 } }; mermaid.initialize(config); window.MathJax = { tex: { tagSide: \"right\", tagIndent: \".8em\", multlineWidth: \"85%\", tags: \"ams\" }, options: { ignoreHtmlClass: 'tex2jax_ignore', processHtmlClass: 'tex2jax_process', renderActions: { find: [10, function (doc) { for (const node of document.querySelectorAll('script[type^=\"math/tex\"]')) { const display = !!node.type.match(/; *mode=display/); const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display); const text = document.createTextNode(''); const sibling = node.previousElementSibling; node.parentNode.replaceChild(text, node); math.start = {node: text, delim: '', n: 0}; math.end = {node: text, delim: '', n: 0}; doc.math.push(math); if (sibling && sibling.matches('.MathJax_Preview')) { sibling.parentNode.removeChild(sibling); } } }, ''] } } };","title":"Test page"},{"location":"misc/test-page/#arithmatex","text":"E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i,j}w_{ij}v_i h_j - \\sum_i b_i v_i - \\sum_j c_j h_j E(\\mathbf{v}, \\mathbf{h}) = -\\sum_{i,j}w_{ij}v_i h_j - \\sum_i b_i v_i - \\sum_j c_j h_j 3 < 4 3 < 4 \\begin{align} p(v_i=1|\\mathbf{h}) & = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\ p(h_j=1|\\mathbf{v}) & = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right) \\end{align} \\begin{align} p(v_i=1|\\mathbf{h}) & = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\ p(h_j=1|\\mathbf{v}) & = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right) \\end{align}","title":"Arithmatex"},{"location":"misc/test-page/#carets","text":"Testing carets","title":"Carets"},{"location":"misc/test-page/#critic","text":"Don't go around saying to people that the world owes you a living. The world owes you nothing. It was here first. One Only one thing is impossible for God: To find any sense in any copyright law on the planet. Truth is stranger than fiction strange but true , but it is because Fiction is obliged to stick to possibilities; Truth isn\u2019t. Don't act so surprised, Your Queen Highness @gsw sounds better . You weren't on any mercy mission this time.","title":"Critic"},{"location":"misc/test-page/#details","text":"Open styled details Nested details! And more content again. Success Content. Warning Content.","title":"Details"},{"location":"misc/test-page/#emoji","text":"","title":"Emoji"},{"location":"misc/test-page/#escapeall","text":"We can escape everything! \u2764\ud83d\ude04","title":"EscapeAll"},{"location":"misc/test-page/#highlight","text":"$a = array ( \"foo\" => 0 , \"bar\" => 1 );","title":"Highlight"},{"location":"misc/test-page/#inlinehilite","text":"Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; .","title":"InlineHilite"},{"location":"misc/test-page/#inline-math","text":"p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"inline math"},{"location":"misc/test-page/#keys","text":"Ctrl \uff0b Alt \uff0b Del","title":"Keys"},{"location":"misc/test-page/#auto-linking","text":"Just paste links directly in the document like this: google.com . Or even an email address: fake.email@email.com .","title":"auto linking"},{"location":"misc/test-page/#mentions","text":"GitHub: @aclk GitLab: @aclk BitBucket: @aclk Twitter: @aqingo","title":"mentions"},{"location":"misc/test-page/#repo-mentions","text":"facelessuser/pymdown-extensions pycqa/flake8","title":"repo mentions"},{"location":"misc/test-page/#issues-and-prs","text":"Issue #1 Issue backrefs#1 Issue Python-Markdown/markdown#1 Issue pycqa/flake8#385 Pull request !13 Pull request backrefs!4 Pull request Python-Markdown/markdown!598 Pull request pycqa/flake8!213","title":"Issues and PRs"},{"location":"misc/test-page/#commits","text":"181c06d backrefs@cb4ecc5 Python-Markdown/markdown@de5c696 pycqa/flake8@8acf55e0","title":"commits"},{"location":"misc/test-page/#compare","text":"e2ed7e0...90b6fb8 backrefs@88c6238...cb4ecc5 Python-Markdown/markdown@007bd2a...de5c696 pycqa/flake8@1ecf9700...9bea7576","title":"compare"},{"location":"misc/test-page/#link-shorteners-external","text":"@facelessuser facelessuser/pymdown-extensions pycqa/flake8#385 mrabarnett/mrab-regex#260","title":"link shorteners - external"},{"location":"misc/test-page/#internal","text":"facelessuser/pymdown-extensions#1 facelessuser/pymdown-extensions!13 facelessuser/pymdown-extensions@3f6b07a facelessuser/pymdown-extensions@e2ed7e0...90b6fb8 facelessuser/Rummage@181c06d","title":"internal"},{"location":"misc/test-page/#mark","text":"mark me smart==mark","title":"Mark"},{"location":"misc/test-page/#progress-bars","text":"0% 5% 25% 45% 65% 85% 100% 85% 100%","title":"Progress Bars"},{"location":"misc/test-page/#smart-symbols","text":"\u2122 \u00a9 \u00ae \u2105 \u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc, etc. 1 st 2 nd etc.","title":"Smart Symbols"},{"location":"misc/test-page/#snippets","text":"","title":"Snippets"},{"location":"misc/test-page/#from-snip-1","text":"from snip 1","title":"from snip 1"},{"location":"misc/test-page/#from-snip-2","text":"from snip 2","title":"from snip 2"},{"location":"misc/test-page/#superfences","text":"Injecting Classes import hello_world Non-Pygments Injecting Classes import hello_world Tabbed fences Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main ( void ) { printf ( \"hello, world \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello, world! \\n \" ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello, world!\" ); } } in context ============================================================ T Tp Sp D Dp S D7 T ------------------------------------------------------------ A F#m Bm E C#m D E7 A A# Gm Cm F Dm D# F7 A# B\u266d Gm Cm F Dm E\u266dm F7 B\u266d import foo.bar import foo.bar \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz Sequence Diagram Example sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts <br/>prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good!","title":"superfences"},{"location":"misc/test-page/#tabbed","text":"Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b","title":"Tabbed"},{"location":"misc/test-page/#two-separate-tab-sets","text":"Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b Tab A Different tab set. Tab B More content.","title":"two separate tab sets"},{"location":"misc/test-page/#tasklist","text":"Task List item 1 item A item B more text item a item b item c item C item 2 item 3","title":"Tasklist"},{"location":"misc/test-page/#tilde","text":"Delete: Delete me Subscript: CH 3 CH 2 OH text a subscript var config = { startOnLoad: false, theme: null, flowchart: { htmlLabels: false }, sequence: { useMaxWidth: false }, class: { textHeight: 16, dividerMargin: 16 } }; mermaid.initialize(config); window.MathJax = { tex: { tagSide: \"right\", tagIndent: \".8em\", multlineWidth: \"85%\", tags: \"ams\" }, options: { ignoreHtmlClass: 'tex2jax_ignore', processHtmlClass: 'tex2jax_process', renderActions: { find: [10, function (doc) { for (const node of document.querySelectorAll('script[type^=\"math/tex\"]')) { const display = !!node.type.match(/; *mode=display/); const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display); const text = document.createTextNode(''); const sibling = node.previousElementSibling; node.parentNode.replaceChild(text, node); math.start = {node: text, delim: '', n: 0}; math.end = {node: text, delim: '', n: 0}; doc.math.push(math); if (sibling && sibling.matches('.MathJax_Preview')) { sibling.parentNode.removeChild(sibling); } } }, ''] } } };","title":"tilde"},{"location":"writing/diagrams/","text":"Online Diagramming Tools \u00b6 Since I don't have a licence for Visio I tend to use the popular drawing tool diagrams.net which is an excellent and highly flexible open source, online, desktop and container-deployable diagramming tool. More about diagrams.net diagrams.net used to be called draw.io . It lets you create a wide range of diagrams, from simple tree and flow diagrams, to highly technical network, rack and electrical diagrams. It\u2019s a free, online diagram editor and viewer with a wide variety of shapes, icons, connector and templates to help you get started quickly. It\u2019s also feature-rich\u2013experienced diagrammers will feel at home. It aims to provide free, high quality diagramming software for everyone, and disrupt the diagramming industry in the process. You can deploy diagrams.net in a docker container for secure diagramming in your company behind your firewall. If you're a user of diagrams.net / draw.io then have a look at my page on how to Import Shapes into Draw.io Diagrams-as-Code \u00b6 Mermaid.js is a syntax that allows you to use text to describe and automatically generate diagrams. You can generate flow charts, UML diagrams, pie charts, Gantt charts, and more. As a developer, I prefer to describe data structures and processes using text wherever I can so that I can more easily use version control systems to track the history of my diagrams. You can use the Mermaid Live Editor to learn the syntax. Amazingly, Mermaid can also be imported into Diagrams.net : Use Mermaid syntax to create diagrams in Diagrams.net Diagramming in Markdown \u00b6 Having learned the Mermaid syntax, I can also used it on my website without exporting diagrams into an image file first, or taking a screenshot! So that this! ```mermaid graph TD A[Client] --> B[Load Balancer] B --> C[Server01] B --> D[Server02] ``` Becomes this! graph TD A[Client] --> B[Load Balancer] B --> C[Server01] B --> D[Server02] As I mention elsewhere , I use MkDocs to generate this website into HTML from a textual format called MarkDown. An extension for Python Markdown called SuperFences allows textual representations of diagrams in formats such as the Mermaid format mentioned above to be rendered directly into the page, simply by adding it to the MarkDown document. SuperFences is part of the PyMdown Extensions project which itself is bundled with Material for Markdown Here are the examples from the Diagrams.net blog post to show that they work in MkDocs too! Mermaid Examples \u00b6 Flowchart \u00b6 graph TD A(Coffee machine <br>not working) --> B{Machine has power?} B -->|No| H(Plug in and turn on) B -->|Yes| C{Out of beans or water?} -->|Yes| G(Refill beans and water) C -->|No| D{Filter warning?} -->|Yes| I(Replace or clean filter) D -->|No| F(Send for repair) Gantt Chart \u00b6 gantt title Example Gantt diagram dateFormat YYYY-MM-DD section Team 1 Research & requirements :done, a1, 2020-03-08, 2020-04-10 Review & documentation : after a1, 20d section Team 2 Implementation :crit, active, 2020-03-25 , 20d Testing :crit, 20d UML Class Diagram \u00b6 classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } var config = { startOnLoad: false, theme: null, flowchart: { htmlLabels: false }, sequence: { useMaxWidth: false }, class: { textHeight: 16, dividerMargin: 16 } }; mermaid.initialize(config);","title":"Technical Diagramming"},{"location":"writing/diagrams/#online-diagramming-tools","text":"Since I don't have a licence for Visio I tend to use the popular drawing tool diagrams.net which is an excellent and highly flexible open source, online, desktop and container-deployable diagramming tool. More about diagrams.net diagrams.net used to be called draw.io . It lets you create a wide range of diagrams, from simple tree and flow diagrams, to highly technical network, rack and electrical diagrams. It\u2019s a free, online diagram editor and viewer with a wide variety of shapes, icons, connector and templates to help you get started quickly. It\u2019s also feature-rich\u2013experienced diagrammers will feel at home. It aims to provide free, high quality diagramming software for everyone, and disrupt the diagramming industry in the process. You can deploy diagrams.net in a docker container for secure diagramming in your company behind your firewall. If you're a user of diagrams.net / draw.io then have a look at my page on how to Import Shapes into Draw.io","title":"Online Diagramming Tools"},{"location":"writing/diagrams/#diagrams-as-code","text":"Mermaid.js is a syntax that allows you to use text to describe and automatically generate diagrams. You can generate flow charts, UML diagrams, pie charts, Gantt charts, and more. As a developer, I prefer to describe data structures and processes using text wherever I can so that I can more easily use version control systems to track the history of my diagrams. You can use the Mermaid Live Editor to learn the syntax. Amazingly, Mermaid can also be imported into Diagrams.net : Use Mermaid syntax to create diagrams in Diagrams.net","title":"Diagrams-as-Code"},{"location":"writing/diagrams/#diagramming-in-markdown","text":"Having learned the Mermaid syntax, I can also used it on my website without exporting diagrams into an image file first, or taking a screenshot! So that this! ```mermaid graph TD A[Client] --> B[Load Balancer] B --> C[Server01] B --> D[Server02] ``` Becomes this! graph TD A[Client] --> B[Load Balancer] B --> C[Server01] B --> D[Server02] As I mention elsewhere , I use MkDocs to generate this website into HTML from a textual format called MarkDown. An extension for Python Markdown called SuperFences allows textual representations of diagrams in formats such as the Mermaid format mentioned above to be rendered directly into the page, simply by adding it to the MarkDown document. SuperFences is part of the PyMdown Extensions project which itself is bundled with Material for Markdown Here are the examples from the Diagrams.net blog post to show that they work in MkDocs too!","title":"Diagramming in Markdown"},{"location":"writing/diagrams/#mermaid-examples","text":"","title":"Mermaid Examples"},{"location":"writing/diagrams/#flowchart","text":"graph TD A(Coffee machine <br>not working) --> B{Machine has power?} B -->|No| H(Plug in and turn on) B -->|Yes| C{Out of beans or water?} -->|Yes| G(Refill beans and water) C -->|No| D{Filter warning?} -->|Yes| I(Replace or clean filter) D -->|No| F(Send for repair)","title":"Flowchart"},{"location":"writing/diagrams/#gantt-chart","text":"gantt title Example Gantt diagram dateFormat YYYY-MM-DD section Team 1 Research & requirements :done, a1, 2020-03-08, 2020-04-10 Review & documentation : after a1, 20d section Team 2 Implementation :crit, active, 2020-03-25 , 20d Testing :crit, 20d","title":"Gantt Chart"},{"location":"writing/diagrams/#uml-class-diagram","text":"classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } var config = { startOnLoad: false, theme: null, flowchart: { htmlLabels: false }, sequence: { useMaxWidth: false }, class: { textHeight: 16, dividerMargin: 16 } }; mermaid.initialize(config);","title":"UML Class Diagram"},{"location":"writing/social-media-sharing/","text":"In order to have my posts look as nice as possible when they are shared on Social Media sites such as LinkedIn and Facebook I needed to work out how to control whether an featured image is shown and what text will be displayed by default. Social Media Link Sharing \u00b6 To show the link sharing working in a couple of Social Media platforms here's how I wanted it to look in Facebook: And here it is in LinkedIn: When you enter a URL the platform picks a layout but it selects the title, a description, and an image by trying to parse information from the HTML page content. Facebook and others will primarily look for this information in Open Graph <meta> tags. Without these tags, the Facebook crawler will do its best to identify content some other way, but for me this was not working very well until I worked out how to add these tags to each content page, and how to have this content be relevant to each page. What is a META tag? W3Schools describes a META tag as follows: Metadata is data (information) about data. The tag provides metadata about the HTML document. Metadata will not be displayed on the page, but will be machine parsable. Meta elements are typically used to specify page description, keywords, author of the document, last modified, and other metadata. The metadata can be used by browsers (how to display content or reload page), search engines (keywords), or other web services. I found this page on the Facebook for developers site about how to use Open Graph META tags The Open Graph protocol The Open Graph protocol enables any web page to become a rich object in a social graph. The Open Graph protocol was originally created at Facebook and is inspired by Dublin Core , link-rel canonical , Microformats , and RDFa There's a great blog post by Neil Patel that explains all about Open Graph meta tags. Because <meta> tags always go inside the <head> element of an <html> page I needed to work out how to manipulate the content of the <head> in each of the pages on this site. Customising generated HTML with MkDocs \u00b6 My site is generated by MkDocs . MkDocs is a static site generator which is a piece of software that I'm using to convert the MarkDown format use to write each of these pages into HTML . It does this for me automatically so that I don't need to write the content in HTML myself - but what if I want to change some aspects of the HTML code? Much of the control over how the Markdown format is converted into HTML using MkDocs is controlled by a Theme. In this section on how to customise the theme you're using (I'm using the excellent Material Theme by @squidfunk ) I worked out that I needed to override one of the template block . In my case, in order to add additional <meta> tags I needed to override the extrahead block. {% raw %} {% extends \"base.html\" %} {% block extrahead %} <meta property=\"og:title\" content=\"{{ page.meta.og_title }}\" /> ... <meta property=\"og:image\" content=\"{{ page.meta.og_image }}\" /> ... {% endblock %} {% endraw %} You can also see from the sample above that I'm referencing the page.meta object which is where I can get data specific to each page and use it to build the content of each meta tag. You can see the full code for the template override file main.html here . Defining document meta-data \u00b6 The way this works is to use the python-markdown Meta-Data extension which adds a syntax for defining meta-data about a Markdown document. The Material theme that I've used makes use of this metadata but I've take this a couple of steps futher by adding a few more of my own custom key-value pairs (such as og_title and og_image ). The metadata is then added to the top of each .md document. The metadata is defined at the beginning of a markdown document and is in YAML format: --- title : Scoop & Co description : Using Scoop (and other Package Managers) to configure my system hero : The 'Scoop' on my personal machine build... date : 2020-02-04 authors : - Bunkei Ka og_title : The 'Scoop' on my personal machine build... page_path : misc/ og_image : media/scoop-update.png --- The path to the image is then used in my template override to build a <meta> tag that looks like this < meta property = \"og:image\" content = \"http://blog.sbux.cf/misc/media/scoop-update.png\" /> Testing it out! \u00b6 To assist with testing the tags in your Markup, Facebook provide a Sharing Debugger . Here you can see that it provides a preview of what link sharing will look like for any URL you provide it to test:","title":"Prettier link sharing with MkDocs"},{"location":"writing/social-media-sharing/#social-media-link-sharing","text":"To show the link sharing working in a couple of Social Media platforms here's how I wanted it to look in Facebook: And here it is in LinkedIn: When you enter a URL the platform picks a layout but it selects the title, a description, and an image by trying to parse information from the HTML page content. Facebook and others will primarily look for this information in Open Graph <meta> tags. Without these tags, the Facebook crawler will do its best to identify content some other way, but for me this was not working very well until I worked out how to add these tags to each content page, and how to have this content be relevant to each page. What is a META tag? W3Schools describes a META tag as follows: Metadata is data (information) about data. The tag provides metadata about the HTML document. Metadata will not be displayed on the page, but will be machine parsable. Meta elements are typically used to specify page description, keywords, author of the document, last modified, and other metadata. The metadata can be used by browsers (how to display content or reload page), search engines (keywords), or other web services. I found this page on the Facebook for developers site about how to use Open Graph META tags The Open Graph protocol The Open Graph protocol enables any web page to become a rich object in a social graph. The Open Graph protocol was originally created at Facebook and is inspired by Dublin Core , link-rel canonical , Microformats , and RDFa There's a great blog post by Neil Patel that explains all about Open Graph meta tags. Because <meta> tags always go inside the <head> element of an <html> page I needed to work out how to manipulate the content of the <head> in each of the pages on this site.","title":"Social Media Link Sharing"},{"location":"writing/social-media-sharing/#customising-generated-html-with-mkdocs","text":"My site is generated by MkDocs . MkDocs is a static site generator which is a piece of software that I'm using to convert the MarkDown format use to write each of these pages into HTML . It does this for me automatically so that I don't need to write the content in HTML myself - but what if I want to change some aspects of the HTML code? Much of the control over how the Markdown format is converted into HTML using MkDocs is controlled by a Theme. In this section on how to customise the theme you're using (I'm using the excellent Material Theme by @squidfunk ) I worked out that I needed to override one of the template block . In my case, in order to add additional <meta> tags I needed to override the extrahead block. {% raw %} {% extends \"base.html\" %} {% block extrahead %} <meta property=\"og:title\" content=\"{{ page.meta.og_title }}\" /> ... <meta property=\"og:image\" content=\"{{ page.meta.og_image }}\" /> ... {% endblock %} {% endraw %} You can also see from the sample above that I'm referencing the page.meta object which is where I can get data specific to each page and use it to build the content of each meta tag. You can see the full code for the template override file main.html here .","title":"Customising generated HTML with MkDocs"},{"location":"writing/social-media-sharing/#defining-document-meta-data","text":"The way this works is to use the python-markdown Meta-Data extension which adds a syntax for defining meta-data about a Markdown document. The Material theme that I've used makes use of this metadata but I've take this a couple of steps futher by adding a few more of my own custom key-value pairs (such as og_title and og_image ). The metadata is then added to the top of each .md document. The metadata is defined at the beginning of a markdown document and is in YAML format: --- title : Scoop & Co description : Using Scoop (and other Package Managers) to configure my system hero : The 'Scoop' on my personal machine build... date : 2020-02-04 authors : - Bunkei Ka og_title : The 'Scoop' on my personal machine build... page_path : misc/ og_image : media/scoop-update.png --- The path to the image is then used in my template override to build a <meta> tag that looks like this < meta property = \"og:image\" content = \"http://blog.sbux.cf/misc/media/scoop-update.png\" />","title":"Defining document meta-data"},{"location":"writing/social-media-sharing/#testing-it-out","text":"To assist with testing the tags in your Markup, Facebook provide a Sharing Debugger . Here you can see that it provides a preview of what link sharing will look like for any URL you provide it to test:","title":"Testing it out!"}]}